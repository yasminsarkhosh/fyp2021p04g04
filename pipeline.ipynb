{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd07ca953050fbd6db0e15562356b1a786d9418e582a94adc67b248dc4dbecd989f",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "7ca953050fbd6db0e15562356b1a786d9418e582a94adc67b248dc4dbecd989f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_labels_and_sentences(textfile, labelfile):\n",
    "\n",
    "    # read textfile and labelfile into two separate dataframes\n",
    "    df_text = pd.read_csv(textfile + '.txt', header=None, skiprows = 0,\n",
    "        names=['tweet'], sep='\\t', quoting=3)\n",
    "    df_labels = pd.read_csv(labelfile + '.txt',header=None, skiprows = 0, names=['label'],\n",
    "        sep='\\t', quoting=3)\n",
    "\n",
    "    index_text = [x for x in range(1, len(df_text.values)+1)]\n",
    "\n",
    "    df_labels.insert(loc=0, column='id', value =index_text)\n",
    "    df_text.insert(loc=0, column='id', value =index_text)\n",
    "    final_df = df_text.merge(df_labels, on='id', how='left')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] \n",
    "        for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) \n",
    "        for words in X])\n",
    "\n",
    "def tokenize_train_test(X_train, X_test):\n",
    "    #Word2Vec\n",
    "    # Word2Vec runs on tokenized sentences\n",
    "    X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "    X_test_tok= [nltk.word_tokenize(i) for i in X_test]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = merging_labels_and_sentences('datasets/offensive/test_text','datasets/offensive/test_labels')\n",
    "\n",
    "data = merging_labels_and_sentences('datasets/offensive/train_text','datasets/offensive/train_labels')\n",
    "\n",
    "data_val = merging_labels_and_sentences('datasets/offensive/val_text','datasets/offensive/val_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('normalizer', StandardScaler()), #Step1 - normalize data\n",
    "    ('clf', LogisticRegression()) #step2 - classifier\n",
    "])\n",
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #X_train, X_test, y_train, y_test = train_test_split(data['tweet'].values,\n",
    "#                                                    data['label'],\n",
    "#                                                    test_size = 0.4,\n",
    "#                                                    random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # print(X_train.shape)\n",
    "# # # print(X_test.shape)\n",
    "# # # print(y_train.shape)\n",
    "# # # print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train[0:2]) # tweets\n",
    "print(X_test[0:2])  # tweets\n",
    "print(y_train[0:2]) # labels\n",
    "print(y_test[0:2]) # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (#W2v)\n",
    "# X_train_tok=[nltk.word_tokenize(i) for i in X_train]\n",
    "# X_test_tok=[nltk.word_tokenize(i) for i in X_test]\n",
    "# model = gensim.models.Word2Vec(X_train_tok,min_count=1)\n",
    "# w2v = dict(zip(model.wv.index_to_key, model.wv))      \n",
    "# modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# # converting text to numerical data using Word2Vec\n",
    "# X_train = modelw.transform(X_train_tok)\n",
    "# X_test = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# scores = cross_validate(pipeline, X_train, y_train)\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "clfs = []\n",
    "clfs.append(LogisticRegression())\n",
    "clfs.append(SVC())\n",
    "clfs.append(SVC())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "\n",
    "for classifier in clfs:\n",
    "    pipeline.set_params(clf = classifier)\n",
    "    scores = cross_validate(pipeline, X_train, y_train)\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('-----------------------------------')\n",
    "    for key, values in scores.items():\n",
    "            print(key,' mean ', values.mean())\n",
    "            print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "pipeline.set_params(clf= SVC())\n",
    "pipeline.steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cv_grid = GridSearchCV(pipeline, param_grid = {\n",
    "#     'clf__kernel' : ['linear', 'rbf'],\n",
    "#     'clf__C' : np.linspace(0.1,1.2,12)\n",
    "# })\n",
    "\n",
    "# cv_grid.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = cv_grid.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_predict)\n",
    "print('Accuracy of the best classifier after CV is %.3f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "#creating a function to encapsulate preprocessing, to mkae it easy to replicate on  submission data\n",
    "def processing(df):\n",
    "    #lowering and removing punctuation\n",
    "    df['processed'] = df['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n",
    "    \n",
    "    #numerical feature engineering\n",
    "    #total length of sentence\n",
    "    df['length'] = df['processed'].apply(lambda x: len(x))\n",
    "    #get number of words\n",
    "    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n",
    "    df['words_not_stopword'] = df['processed'].apply(lambda x: len([t for t in x.split(' ') if t not in stopWords]))\n",
    "    #get the average word length\n",
    "    df['avg_word_length'] = df['processed'].apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) if len([len(t) for t in x.split(' ') if t not in stopWords]) > 0 else 0)\n",
    "    #get the average word length\n",
    "    df['commas'] = df['tweet'].apply(lambda x: x.count(','))\n",
    "\n",
    "    return(df)\n",
    "\n",
    "data_processed = processing(data)\n",
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features= [c for c in data_processed.columns.values if c  not in ['id','tweet','label']]\n",
    "numeric_features= [c for c in data_processed.columns.values if c  not in ['id','tweet','label','processed']]\n",
    "target = 'label'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_processed[features], data_processed[target], test_size=0.33, random_state=42)\n",
    "X_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='processed')),\n",
    "                ('tfidf', TfidfVectorizer( stop_words='english'))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "x = length.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas)])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "preds = pipeline.predict(X_test)\n",
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters = { 'features__text__tfidf__max_df': [0.9, 0.95],\n",
    "                    'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "                   'classifier__max_depth': [50, 70],\n",
    "                    'classifier__min_samples_leaf': [1,2]\n",
    "                  }\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=3)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#refitting on entire training data using best settings\n",
    "clf.refit\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)\n",
    "\n",
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_test.to_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission =  pd.read_csv('test_labels.csv')\n",
    "#preprocessing\n",
    "submission = processing(submission)\n",
    "predictions = clf.predict_proba(submission)\n",
    "\n",
    "preds = pd.DataFrame(data=predictions, columns = clf.best_estimator_.named_steps['classifier'].classes_)\n",
    "\n",
    "#generating a submission file\n",
    "result = pd.concat([submission[['id']], preds], axis=1)\n",
    "result.set_index('id', inplace = True)\n",
    "result.head()\n",
    "result.to_csv('final_prediction_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Put this when it's called\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ignore warning prints\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "source": [
    "## Reading in the data\n",
    "\n",
    "- data: the train data set\n",
    "- data_test: the test data set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Functions for the classification pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table for missing data analysis\n",
    "def draw_missing_data_table(df):\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return missing_data\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Plot validation curve\n",
    "def plot_validation_curve(estimator, title, X, y, param_name, param_range, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    train_scores, test_scores = validation_curve(estimator, X, y, param_name, param_range, cv)\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plt.plot(param_range, train_mean, color='r', marker='o', markersize=5, label='Training score')\n",
    "    plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='r')\n",
    "    plt.plot(param_range, test_mean, color='g', linestyle='--', marker='s', markersize=5, label='Validation score')\n",
    "    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='g')\n",
    "    plt.grid() \n",
    "    plt.xscale('log')\n",
    "    plt.legend(loc='best') \n",
    "    plt.xlabel('Parameter') \n",
    "    plt.ylabel('Score') \n",
    "    plt.ylim(ylim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = data\n",
    "df_raw = df.copy()  # Save original data set, just in case.\n",
    "df_raw_val = data_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df.describe().to_csv('descriptive_statistics_trainingset.csv')"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "### There are three aspects that usually catch my attention when I analyse descriptive statistics:\n",
    "\n",
    "- **Min and max values.** This can give us an idea about the range of values and is helpful to detect outliers. In our case, all the min and max values seem reasonable and in a reasonable range of values. The only exception could eventually be the max value of 'Fare', but for now we will leave it as it is.\n",
    "- **Mean and standard deviation.** The mean shows us the central tendency of the distribution, while the standard deviation quantifies its amount of variation. For example, a low standard deviation suggests that data points tend to be close to the mean. Giving a quick look to our values, there's nothing that looks like obviously wrong.\n",
    "- **Count.** This is important to give us a first perception about the volume of missing data. Here, we can see that some 'Age' data is missing.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse missing data\n",
    "draw_missing_data_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data types\n",
    "df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df data consists of data from the training set only, which are then split into 'train' and 'test'\n",
    "\n",
    "X = df['tweet']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "print('Inputs: \\n', X_train.head())\n",
    "print('Outputs: \\n', y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tf-Idf)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression\n",
    "logreg = LogisticRegression(max_iter = 1000, solver='lbfgs')\n",
    "logreg.fit(X_train_vectors_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance\n",
    "scores = cross_val_score(logreg, X_train_vectors_tfidf, y_train, cv=3)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing model performance\n",
    "# Plot learning curves\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "cv = 3\n",
    "plot_learning_curve(logreg, title, X_train_vectors_tfidf, y_train, ylim=(0.63, 0.95), cv=cv, n_jobs=3);"
   ]
  },
  {
   "source": [
    "***\n",
    "\n",
    "**Discussion of results**\n",
    "\n",
    "- We see a large gab between the training score and the validation score.\n",
    "\n",
    "- _**Thus, the learning curve of the training score and the validation score shows an overfitting in the model.**_ \n",
    "\n",
    "- A solution for the overfitting would be:\n",
    "    1. Reduce the complexity of the model and/or\n",
    "    2. Collect more data\n",
    "\n",
    "- The final score is 0.749, which means our model does better predictions than a flip-a-coin strategy, but is still far from being an useful model.\n",
    "\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "\n",
    "**Learning curves in a nutshell:**\n",
    "\n",
    "- Learning curves allow us to _diagnose_ if the is _**overfitting**_ or _**underfitting**_.\n",
    "\n",
    "- When the _**model overfits**_, it means that it _**performs well on the training set, but not not on the validation set**_. Accordingly, the model is not able to generalize to unseen data. If the model is overfitting, the learning curve will present a gap between the training and validation scores. Two common solutions for overfitting are reducing the complexity of the model and/or collect more data.\n",
    "\n",
    "- On the other hand, _**underfitting means that the model is not able to perform well in either training or validations sets**_. In those cases, the _**learning curves will converge to a low score value**_. When the model underfits, _gathering more data_ is _**not helpful**_ because the _**model**_ is already _**not**_ being able to _**learn the training data**_. \n",
    "\n",
    "---------------------------------------------------------------------\n",
    "** <center> Therefore, the best approaches for these cases are to improve the model (e.g., tuning the hyperparameters) or to improve the quality of the data (e.g., collecting a different set of features).</center>**\n",
    "\n",
    "***\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curve\n",
    "title = 'Validation Curve (Logistic Regression)'\n",
    "param_name = 'C' \n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] \n",
    "cv = 3\n",
    "test = plot_validation_curve(estimator=logreg, title=title, X=X_train_vectors_tfidf, y=y_train, param_name=param_name,\n",
    "                      ylim=(0.5, 1.01), param_range=param_range);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "\n",
    "df_test = pd.DataFrame(columns = ['C_parameter'])\n",
    "df_test['C_parameter'] = C_param_range\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "j = 0\n",
    "for i in C_param_range:\n",
    "    \n",
    "    # Apply logistic regression model to training data\n",
    "    lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n",
    "    lr.fit(X_train_vectors_tfidf,y_train)\n",
    "    \n",
    "    # Predict using model\n",
    "    y_pred_sepal = lr.predict(X_test_vectors_tfidf)\n",
    "    \n",
    "    # Saving accuracy score in table\n",
    "    df_test = accuracy_score(y_test,y_pred_sepal)\n",
    "    j += 1\n",
    "    \n",
    "    # Printing decision regions\n",
    "    plt.subplot(3,2,j)\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    plot_validation_curve(estimator=logreg, title=title, X=X_train_vectors_tfidf, y=y_train, param_name=param_name,\n",
    "                      ylim=(0.5, 1.01), param_range=param_range);\n",
    "    plt.title('C = %s'%i)"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "**Validation curves in a nutshell:**\n",
    "\n",
    "Validation curves are a tool that we can use to improve the performance of our model. It counts as a way of tuning our hyperparameters.\n",
    "They are different from the learning curves. Here, the goal is to see how the model parameter impacts the training and validation scores. This allow us to choose a different value for the parameter, to improve the model.\n",
    "Once again, if there is a gap between the training and the validation score, the model is probably overfitting. In contrast, if there is no gap but the score value is low, we can say that the model underfits.\n",
    "\n",
    "**Discussion of our results:**\n",
    "\n",
    "_The figure shows that there is a huge difference in model's performance. Note that in a logistic regression, C is the only model parameter that we can change (see scikit-learn documentation)._ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Splitting into: \n",
    "#### Training and Test from the two provided text files for training data set and test data set\n",
    "\n",
    "_The previous results of model performance was executed from the training data set only, from which was split into a training and the testing part. We want to explore if that makes any difference for the same results if we approach same functions with the training and the test data set instead_ \n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = data['tweet']\n",
    "X_test_ = data_test['tweet']\n",
    "y_train_ = data['label']\n",
    "y_test_ = data_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer #for word embedding\n",
    "\n",
    "# (tf-Idf)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf_ = tfidf_vectorizer.fit_transform(X_train_) \n",
    "X_test_vectors_tfidf_ = tfidf_vectorizer.transform(X_test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "print('Inputs: \\n', X_train_.head())\n",
    "print('Outputs: \\n', y_train_.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression\n",
    "logreg_ = LogisticRegression(max_iter = 1000, solver='lbfgs')\n",
    "logreg_.fit(X_train_vectors_tfidf_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance\n",
    "scores_train = cross_val_score(logreg_, X_train_vectors_tfidf_, y_train_, cv=3)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_train), np.std(scores_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing model performance\n",
    "# Plot learning curves\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "cv = 3\n",
    "plot_learning_curve(logreg_, title, X_train_vectors_tfidf_, y_train_, ylim=(0.6, 0.85), cv=cv, n_jobs=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X,y,classifier, test_idx = None,resolution=0.02):\n",
    "    #print(X, y)\n",
    "    \n",
    "    # Initialise the marker types and colors\n",
    "    markers = ('s','x','o','^','v')\n",
    "    colors = ('red','blue','lightgreen','gray','cyan')\n",
    "    color_Map = ListedColormap(colors[:len(np.unique(y))]) #we take the color mapping correspoding to the \n",
    "                                                            #amount of classes in the target data\n",
    "    \n",
    "    # Parameters for the graph and decision surface\n",
    "    x1_min = X[:,0].min() - 1\n",
    "    x1_max = X[:,0].max() + 1\n",
    "    x2_min = X[:,1].min() - 1\n",
    "    x2_max = X[:,1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),\n",
    "                           np.arange(x2_min,x2_max,resolution))\n",
    "    \n",
    "    #Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n",
    "    #Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    # plt.contour(xx1,xx2,alpha=0.4,cmap = color_Map)\n",
    "    # plt.xlim(xx1.min(),xx1.max())\n",
    "    # plt.ylim(xx2.min(),xx2.max())\n",
    "    \n",
    "    # Plot samples\n",
    "    X_test, Y_test = X[test_idx,:], y[test_idx]\n",
    "    print(X_test, Y_test)\n",
    "    \n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n",
    "                    alpha = 0.8, c = color_Map(idx),\n",
    "                    marker = markers[idx], label = cl\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# training samples : \", len(X_train_))\n",
    "print(\"# testing samples : \", len(y_train_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "\n",
    "df_test = pd.DataFrame(columns = ['C_parameter'])\n",
    "df_test['C_parameter'] = C_param_range\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "j = 0\n",
    "for i in C_param_range:\n",
    "    \n",
    "    # Apply logistic regression model to training data\n",
    "    lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n",
    "    lr.fit(X_train_vectors_tfidf_,y_train_)\n",
    "    \n",
    "    # Predict using model\n",
    "    y_pred_sepal_ = lr.predict(X_test_vectors_tfidf_)\n",
    "    \n",
    "    # Saving accuracy score in table\n",
    "    df_test = accuracy_score(y_test_, y_pred_sepal_)\n",
    "    j += 1\n",
    "    \n",
    "    # Printing decision regions\n",
    "    plt.subplot(3,2,j)\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    \n",
    "    plot_validation_curve(estimator=logreg_, title=title, X=X_train_vectors_tfidf_, y=y_train_, param_name=param_name,\n",
    "                      ylim=(0.5, 1.01), param_range=param_range);\n",
    "    plt.title('C = %s'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curve\n",
    "title = 'Validation Curve (Logistic Regression)'\n",
    "param_name = 'C'\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] \n",
    "cv = 3\n",
    "plot_validation_curve(estimator=logreg_, title=title, X=X_train_vectors_tfidf_, y=y_train_, param_name=param_name,\n",
    "                      ylim=(0.5, 1.01), param_range=param_range);\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "**Validation curves in a nutshell:**\n",
    "\n",
    "Validation curves are a tool that we can use to improve the performance of our model. It counts as a way of tuning our hyperparameters.\n",
    "They are different from the learning curves. Here, the goal is to see how the model parameter impacts the training and validation scores. This allow us to choose a different value for the parameter, to improve the model.\n",
    "Once again, if there is a gap between the training and the validation score, the model is probably overfitting. In contrast, if there is no gap but the score value is low, we can say that the model underfits.\n",
    "\n",
    "**Discussion of our results:**\n",
    "\n",
    "_The figure shows that there is no huge difference in model's performance as far as we choose a C value of 10âˆ’1 or higher. Note that in a logistic regression, C is the only model parameter that we can change (see scikit-learn documentation)._ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## **Summary**\n",
    "\n",
    "There is no major difference from: \n",
    "- splitting the training data set into a training and a testing part or,\n",
    "- splitting into a training set from the provided training data and into a testing set from the provided testing data.\n",
    "\n",
    "***\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "#warnings.warn(CV_Warning, category=FutureWarning)\n",
    "\n",
    "\n",
    "### 1. Use of validation curves for both datasets.\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Apply logistic regression model to training data\n",
    "lr = LogisticRegression(penalty='l2',C = i,random_state = 0)\n",
    "\n",
    "# Plot validation curve\n",
    "train_scores, test_scores = validation_curve(estimator=lr\n",
    "                                                            ,X=X_train_vectors_tfidf_\n",
    "                                                            ,y=y_train_\n",
    "                                                            ,param_name='C'\n",
    "                                                            ,param_range=C_param_range\n",
    "                                                            )\n",
    "\n",
    "train_mean = np.mean(train_scores,axis=1)\n",
    "train_std = np.std(train_scores,axis=1)\n",
    "test_mean = np.mean(test_scores,axis=1)\n",
    "test_std = np.std(test_scores,axis=1)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(C_param_range\n",
    "            ,train_mean\n",
    "            ,color='blue'\n",
    "            ,marker='o'\n",
    "            ,markersize=5\n",
    "            ,label='training accuracy')\n",
    "    \n",
    "plt.plot(C_param_range\n",
    "            ,test_mean\n",
    "            ,color='green'\n",
    "            ,marker='x'\n",
    "            ,markersize=5\n",
    "            ,label='test accuracy') \n",
    "    \n",
    "plt.xlabel('C_parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.5,1.5])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Disables ConvergenceWarning\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "import sys\n",
    "\n",
    "## Get score using original model\n",
    "logreg_ = LogisticRegression(C=1, solver='lbfgs')\n",
    "logreg_.fit(X_train_vectors_tfidf_, y_train_)\n",
    "scores = cross_val_score(logreg_, X_train_vectors_tfidf_, y_train_, cv=3)\n",
    "with open('CV_accuracy_orginal.txt', 'w') as o:\n",
    "\n",
    "    print('CV accuracy (original): %.3f +/- %.3f' % (np.mean(scores), \n",
    "    np.std(scores)), file=o)\n",
    "    o.close()\n",
    "    highest_score = np.mean(scores)\n",
    "\n",
    " ## Get score using models with feature selection\n",
    "for i in range(1, 50, 1):\n",
    "    # Select i features\n",
    "    select = SelectKBest(score_func=chi2, k=i)\n",
    "    select.fit(X_train_vectors_tfidf_, y_train_)\n",
    "   \n",
    "    X_train_poly_selected = select.transform(X_train_vectors_tfidf_)\n",
    "\n",
    "    # Model with i features selected\n",
    "    logreg_.fit(X_train_poly_selected, y_train_)\n",
    "    scores = cross_val_score(logreg_, X_train_poly_selected, y_train_,  \n",
    "    cv=3)\n",
    "    with open('CV_accuracy.txt', 'w') as f:\n",
    "        print('CV accuracy (number of features = %i): %.3f +/- %.3f' %\n",
    "        (i, np.mean(scores), np.std(scores)), file=f)\n",
    "                                                                \n",
    "                                                                \n",
    "\n",
    "        # Save results if best score\n",
    "        if np.mean(scores) > highest_score:\n",
    "            highest_score = np.mean(scores)\n",
    "            std = np.std(scores)\n",
    "            k_features_highest_score = i\n",
    "        elif np.mean(scores) == highest_score:\n",
    "            if np.std(scores) < std:\n",
    "                highest_score = np.mean(scores)\n",
    "                std = np.std(scores)\n",
    "                k_features_highest_score = i\n",
    "     \n",
    "    f.close()\n",
    "\n",
    " \n",
    "print('Number of features when highest score: %i' % k_features_highest_score)\n"
   ]
  },
  {
   "source": [
    "# FIT MODEL FOR BEST FEATURE COMBINATION\n",
    "\n",
    "_The Highly Accurate Model_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "select = SelectKBest(score_func=chi2, k=k_features_highest_score)\n",
    "select.fit(X_train_vectors_tfidf_, y_train_)\n",
    "\n",
    "# Fit model\n",
    "logreg = LogisticRegression(C=1) #C = 1 which is on the edge of overfitting the model\n",
    "logreg.fit(X_train_poly_selected, y_train_)\n",
    "\n",
    "# Model performance\n",
    "scores = cross_val_score(logreg_, X_train_poly_selected, y_train_, cv=3)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "cv = 3\n",
    "plot_learning_curve(logreg_, title, X_train_poly_selected, \n",
    "                    y_train_, ylim=(0.65, 0.8), cv=cv, n_jobs=3);\n",
    "\n"
   ]
  },
  {
   "source": [
    "### **Results**\n",
    "***\n",
    "\n",
    "The new model shows no signs of overfitting or underfitting between the training score and validation score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curve\n",
    "\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "\n",
    "# Disables ConvergenceWarning\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "\n",
    "title = 'Validation Curve (Logistic Regression)'\n",
    "param_name = 'C'\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] \n",
    "cv = 3\n",
    "plot_validation_curve(estimator=logreg_, title=title, X=X_train_poly_selected, y=y_train_, \n",
    "                      param_name=param_name, ylim=(0.6, 0.85), param_range=param_range);\n",
    "\n"
   ]
  },
  {
   "source": [
    "### **Results**\n",
    "\n",
    "***\n",
    "The C parameter is set to 1 which is on the edge of overfitting. There is also no signs of underfitting the model as the model performs well (no gaps between the two scores)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Submit Predictions "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data set\n",
    "df_test = data_test\n",
    "\n",
    "# copy of data_test\n",
    "df_test_raw = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = logreg.predict(X_train_poly_selected)\n",
    "predictions\n",
    "\n",
    "\n",
    "# Generate submission file\n",
    "submission = pd.DataFrame({ 'tweet': X_train_,\n",
    "                            'predicted': predictions})\n",
    "\n",
    "#submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tweets predicted as offensive:', sum(submission['predicted'] == 1))\n",
    "print('Number of tweets predicted as non-offensive:', sum(submission['predicted'] == 0))\n"
   ]
  },
  {
   "source": [
    "## Conclusion\n",
    "\n",
    "The aim of this sections was to improve data quality through feature extraction techniques and exploratory data analysis. Thus, explored different techniques to improve our model performance and data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = processing(data)\n",
    "data_test_processed = processing(data_test)\n",
    "data_processed.head()\n",
    "data_test_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features= [c for c in data_processed.columns.values if c  not in ['id','tweet','label']]\n",
    "numeric_features= [c for c in data_processed.columns.values if c  not in ['id','tweet','label','processed']]\n",
    "target = 'label'\n",
    "\n",
    "features_test= [c for c in data_test_processed.columns.values if c  not in ['id','tweet','label']]\n",
    "numeric_features= [c for c in data_test_processed.columns.values if c  not in ['id','tweet','label','processed']]\n",
    "target_test = 'label'\n",
    "\n",
    "\n",
    "\n",
    "X_train_pro = data_processed[features] \n",
    "X_test_pro = data_test_processed[features_test]\n",
    "y_train_pro = data_processed[target]\n",
    "y_test_pro = data_test_processed[target_test]\n",
    "\n",
    "\n",
    "X_train_pro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='processed')),\n",
    "                ('tfidf', TfidfVectorizer( stop_words='english'))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(X_train_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas)])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train_pro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train_pro, y_train_pro)\n",
    "\n",
    "preds_pro = pipeline.predict(X_test_pro)\n",
    "\n",
    "print(np.mean(preds_pro == y_test_pro))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters_pro = { 'features__text__tfidf__max_df': [0.9, 0.95],\n",
    "                    'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "                   'classifier__max_depth': [50, 70],\n",
    "                    'classifier__min_samples_leaf': [1,2]\n",
    "                  }\n",
    "clf_pro = GridSearchCV(pipeline, hyperparameters_pro, cv=3)\n",
    " \n",
    "# Fit and tune model\n",
    "clf_pro.fit(X_train_pro, y_train_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pro.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refitting on entire training data using best settings\n",
    "clf_pro.refit\n",
    "\n",
    "preds_clf_pro = clf_pro.predict(X_test_pro)\n",
    "probs_clf_pro = clf_pro.predict_proba(X_test_pro)\n",
    "\n",
    "np.mean(preds_clf_pro == y_test_pro)\n",
    "\n",
    "data.to_csv('prediction_train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pro =  pd.read_csv('prediction_train_processed.csv')\n",
    "\n",
    "#preprocessing\n",
    "submission_pro = processing(submission_pro)\n",
    "predictions_pro = clf_pro.predict_proba(submission_pro)\n",
    "\n",
    "preds_pro_df = pd.DataFrame(data=predictions_pro, columns = clf_pro.best_estimator_.named_steps['classifier'].classes_)\n",
    "\n",
    "#generating a submission file\n",
    "result = pd.concat([submission_pro[['id']], preds_pro_df], axis=1)\n",
    "result.set_index('id', inplace = True)\n",
    "result.head()\n",
    "result.to_csv('final_prediction_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(pipeline, X_train_pro, y_train_pro)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('normalizer', StandardScaler(with_mean=False)), #Step1 - normalize data\n",
    "    ('clf', LogisticRegression()) #step2 - classifier\n",
    "])\n",
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf  = TfidfVectorizer()\n",
    "X_train_pro_tf = tfidf.fit_transform(X_train_processed)\n",
    "X_test_pro_tf = tfidf.fit_transform(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "clfs = []\n",
    "clfs.append(LogisticRegression())\n",
    "clfs.append(SVC())\n",
    "clfs.append(SVC())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "\n",
    "with open('descriptive_statistics_classifiers.csv', 'w') as f:\n",
    "    for classifier in clfs:\n",
    "        pipeline.set_params(clf = classifier)\n",
    "        scores = cross_validate(pipeline, X_train_pro, y_train_pro)\n",
    "        print('---------------------------------', file=f)\n",
    "        print(str(classifier), file=f)\n",
    "        print('-----------------------------------', file=f)\n",
    "        for key, values in scores.items():\n",
    "                print(key,' mean ', values.mean(), file=f)\n",
    "                print(key,' std ', values.std(), file=f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}