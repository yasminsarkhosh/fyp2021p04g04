{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1.1: CLEANING DATA SET AND TOKENIZATION\n",
    "_ removing any unwanted characters and splitting text files into words _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yasmin/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/yasmin/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to /home/yasmin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import operator\n",
    "from collections import Counter\n",
    "import collections\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pathlib\n",
    "import string\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "import collections\n",
    "import nltk.tokenize\n",
    "import pickle\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text file into sentences\n",
    "from nltk import sent_tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def token_sentences(text):\n",
    "    sentences = sent_tokenize(text)   \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# tokenize text file into words\n",
    "def tokenization(text):\n",
    "\n",
    "    tokens = word_tokenize(text[1:])\n",
    "    words = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Vocabulary\n",
    "_ step 1: Tokenize the words into characters in the corpus and append </w> at the end of every word_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Looking through vocabulary lists can help you find problems\n",
    "(especially tokens that only occur once or twice)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a dataframe into a single list \n",
    "#text is split into words defined by their space inbetween\n",
    "#words are inserted into list \n",
    "\n",
    "def words_list(text):\n",
    "    #words are inserted into list \n",
    "    corpus=[]\n",
    "    for row in text:\n",
    "        tokens = row[0].split(\" \")\n",
    "        for token in tokens:\n",
    "            corpus.append(token)\n",
    "    \n",
    "    \n",
    "    def vocabulary_list(corpus):\n",
    "        #initlialize the vocabulary\n",
    "        vocab = list(set(\" \".join(corpus)))\n",
    "        vocab.remove(' ')\n",
    "        return vocab\n",
    "      \n",
    "    \n",
    "    def split_words_char(corpus):\n",
    "        #split the word into characters\n",
    "        corpus = [\" \".join(token) for token in corpus]\n",
    "\n",
    "        #appending </w>\n",
    "        corpus=[token+' </w>' for token in corpus]\n",
    "        return corpus\n",
    "        \n",
    "    x,y = split_words_char(corpus), vocabulary_list(corpus)\n",
    "\n",
    "\n",
    "    return x,y\n",
    "\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "def bag_of_words(text):\n",
    "    word2count = {}\n",
    "    for data in text:\n",
    "        words = nltk.word_tokenize(data)\n",
    "        for word in words:  \n",
    "            if word not in word2count.keys():\n",
    "                word2count[word] = 1\n",
    "            else:\n",
    "                word2count[word] += 1\n",
    "    return word2count   \n",
    "\n",
    "# Frequency of words in BAG\n",
    "def freq_words(word2count):\n",
    "    import heapq\n",
    "    freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
    "    return freq_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "# convert all words into lower cases\n",
    "# remove stop words\n",
    "\n",
    "def preprocess_text(words):\n",
    "    #delete punctuations\n",
    "    punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    #convert all words into lower cases\n",
    "    words = [word.translate(punctuation_table).lower() for word in words]\n",
    "    \n",
    "    #remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "# cleaning sentences within data frame\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", elem))  \n",
    "    return df\n",
    "\n",
    "\n",
    "# remove only punctuations\n",
    "def del_punctuations(words):\n",
    "\n",
    "    punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    words = [word.translate(punctuation_table).lower() for word in words]\n",
    "    return words\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# remove stop words\n",
    "def stop_words(words):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#lemmatization of words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens, sentences, average tokens, total unique tokens, total number of tokens after cleaning\n",
    "\n",
    "def basic_statistics(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    tokens = word_tokenize(text[1:])\n",
    "    words = [token.lower() for token in tokens if token.isalpha()]\n",
    "    average_tokens = round(len(words)/len(sents))\n",
    "    unique_tokens = set(words)\n",
    "    token_ratio = round(len(unique_tokens)/len(tokens),3)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    final_tokens = []\n",
    "    for each in words:\n",
    "        if each not in stop_words:\n",
    "            final_tokens.append(each) \n",
    "    \n",
    "    return len(tokens), len(sents), average_tokens, len(unique_tokens), token_ratio, len(final_tokens)\n",
    "\n",
    "\n",
    "#returns frequency of each word\n",
    "def word_frequency(words):\n",
    "    frequency_words = collections.Counter(words)\n",
    "    \n",
    "    #convert counter object to dictionary\n",
    "    frequency_words_dict = dict(frequency_words)\n",
    "    res = dict(sorted(frequency_words_dict.items(), key=lambda item: item[1]))\n",
    "    return res\n",
    "\n",
    "#returns top 20 most common words\n",
    "def top_20_most_common_words(freq_words):\n",
    "    res = dict(Counter(freq_words).most_common(20))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Tokenization of text files: Offensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in txt files: offensive/val_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['user', '@user', 'WiiU', 'is', 'not', 'even', 'a', 'real', 'console.', '@user', '@user', '@user', 'If', 'he', 'is', 'from', 'AZ', 'I', 'would', 'put', 'my', 'money', 'on', 'sex', 'with', 'underage', 'kids.', '@user', 'I', 'thought', 'Canada', 'had', 'strict', 'gun', 'control.', 'Help', 'me', 'understand', 'what', 'is', 'happening.', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', 'Following', 'all', '#Maga', 'patriots', 'please', 'follow', 'back', 'üëç', '#LionsDen', 'ü¶Å', '#MAGA2KAG', 'üá∫üá∏', '1', 'Minute', 'of', 'Truth:', 'Gun', 'Control', 'via', '@user', '@user', '@user', '@user', 'We', 'could', 'help', 'if', 'you', 'are', 'London', 'based', 'üòä', '@user', '@user', 'There', 'r', '65', 'million', 'that', 'can', 'sign', 'to', 'the']\n"
     ]
    }
   ],
   "source": [
    "file_path_val = pathlib.Path('datasets/offensive/val_text.txt')\n",
    "\n",
    "with open(file_path_val, 'r',encoding=\"utf8\") as f:\n",
    "    text_val = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "words_val_txt = text_val[1:].split()\n",
    "print(words_val_txt[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['user', 'Bono...', 'who', 'cares.', 'Soon', 'people', 'will', 'understand', 'that', 'they', 'gain', 'nothing', 'from', 'following', 'a', 'phony', 'celebrity.', 'Become', 'a', 'Leader', 'of', 'your', 'people', 'instead', 'or', 'help', 'and', 'support', 'your', 'fellow', 'countrymen.', '@user', 'Eight', 'years', 'the', 'republicans', 'denied', 'obama‚Äôs', 'picks.', 'Breitbarters', 'outrage', 'is', 'as', 'phony', 'as', 'their', 'fake', 'president.', '@user', 'Get', 'him', 'some', 'line', 'help.', 'He', 'is', 'gonna', 'be', 'just', 'fine.', 'As', 'the', 'game', 'went', 'on', 'you', 'could', 'see', 'him', 'progressing', 'more', 'with', 'his', 'reads.', 'He', 'brought', 'what', 'has', 'been', 'missing.', 'The', 'deep', 'ball', 'presence.', 'Now', 'he', 'just', 'needs', 'a', 'little', 'more', 'time', '@user', '@user', 'She', 'is', 'great.', 'Hi', 'Fiona!', '@user']\n"
     ]
    }
   ],
   "source": [
    "file_path_train = pathlib.Path('datasets/offensive/train_text.txt')\n",
    "\n",
    "\n",
    "with open(file_path_train, 'r',encoding=\"utf8\") as f:\n",
    "    text_train = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "words_train_txt = text_train[1:].split()\n",
    "\n",
    "print(words_train_txt[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ibelieveblaseyford', 'is', 'liar', 'she', 'is', 'fat', 'ugly', 'libreal', '#snowflake', 'she', 'sold', 'her', 'herself', 'to', 'get', 'some', 'cash', '!!', 'From', 'dems', 'and', 'Iran', '!', 'Why', 'she', 'spoke', 'after', '#JohnKerryIranMeeting', '?', '@user', '@user', '@user', 'I', 'got', 'in', 'a', 'pretty', 'deep', 'debate', 'with', 'my', 'friend', 'and', 'she', 'told', 'me', 'that', 'latinos', 'for', 'Trump', 'and', 'blacks', 'for', 'Trump', 'were', 'paid', 'supporters', 'üòÇ', 'then', 'I', 'said', 'you', 'mean', 'antifa', 'are', 'paid', 'domestic', 'terrorist', 'and', 'she', 'said', 'No', 'they', 'are', 'anti-fascist', 'then', 'I', 'said', 'they', 'are', 'the', 'fascist', 'are', 'you', 'kidding', 'me?!', '...if', 'you', 'want', 'more', 'shootings', 'and', 'more', 'death,', 'then', 'listen', 'to', 'the', 'ACLU,', 'Black']\n"
     ]
    }
   ],
   "source": [
    "file_path_test = pathlib.Path('datasets/offensive/test_text.txt')\n",
    "\n",
    "\n",
    "with open(file_path_test, 'r',encoding=\"utf8\") as f:\n",
    "    text_test = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "words_test_txt = text_test[1:].split()\n",
    "\n",
    "print(words_test_txt[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words in tokenization for val_text: 30416\nNumber of words in tokenization for val_train: 258224\nNumber of words in tokenization for val_test: 19619\n"
     ]
    }
   ],
   "source": [
    "# total number of tokenization of words for each variable\n",
    "token_val = tokenization(text_val)\n",
    "token_train = tokenization(text_train)\n",
    "token_test = tokenization(text_test)\n",
    "\n",
    "print(f'Number of words in tokenization for val_text: {len(token_val)}')\n",
    "print(f'Number of words in tokenization for val_train: {len(token_train)}')\n",
    "print(f'Number of words in tokenization for val_test: {len(token_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text file into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['@user @user WiiU is not even a real console.', '@user @user @user If he is from AZ I would put my money on sex with underage kids.', '@user I thought Canada had strict gun control.', 'Help me understand what is happening.', \"@user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user Following all #Maga patriots please follow back üëç  #LionsDen ü¶Å  #MAGA2KAG üá∫üá∏ \\n1 Minute of Truth: Gun Control via @user \\n@user @user @user We could help if you are London based üòä \\n@user @user There r 65 million that can sign to the affect that they didn't vote for an asshole.\"]\n['@user Bono... who cares.', 'Soon people will understand that they gain nothing from following a phony celebrity.', 'Become a Leader of your people instead or help and support your fellow countrymen.', '@user Eight years the republicans denied obama‚Äôs picks.', 'Breitbarters outrage is as phony as their fake president.']\n['#ibelieveblaseyford is liar she is fat ugly libreal #snowflake she sold her  herself to get some cash !!', 'From dems and Iran  !', 'Why she spoke after  #JohnKerryIranMeeting ?', '@user @user @user I got in a pretty deep debate with my friend and she told me that latinos for Trump and blacks for Trump were paid supporters üòÇ then I said you mean antifa are paid domestic terrorist and she said No they are  anti-fascist then I said they are the fascist are you kidding me?!', '...if you want more shootings and more death, then listen to the ACLU, Black Lives Matter, or Antifa.']\n"
     ]
    }
   ],
   "source": [
    "sentences_val_txt = token_sentences(text_val)  \n",
    "sentences_train_txt = token_sentences(text_train)\n",
    "sentences_test_txt = token_sentences(text_test)   \n",
    "\n",
    "print(sentences_val_txt[:5])\n",
    "print(sentences_train_txt[:5])\n",
    "print(sentences_test_txt[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of sentences for val_text: 2124\nNumber of sentences for val_train: 18122\nNumber of sentences for val_test: 1214\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of sentences for val_text: {len(sentences_val_txt)}')\n",
    "print(f'Number of sentences for val_train: {len(sentences_train_txt)}')\n",
    "print(f'Number of sentences for val_test: {len(sentences_test_txt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary list for offensive text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary for text_val.txt:\n ['‚¨á', 'ü§¶', 'üôè', 'üêç', 'Y', 'x', '‚ô•', '‚Äú', 'J', 'd', 'üá¶', 'T', 'üéâ', 'üáØ', 'üôà', 'a', '#', '‚ù§', 'i', 'ü§î'] \n\nVocabulary for text_train.txt:\n ['‚¨á', 'üçΩ', 'Y', 'üò£', '‚ù£', 'J', 'üá¶', 'üë¢', 'üöó', 'üåå', '√ü', 'ü§Ø', '·µà', 'üòö', '‚Äù', 'üíâ', 'üî®', 'M', '=', 'ü§†'] \n\nVocabulary for text_test.txt:\n ['üôè', 'Y', 'x', '‚Ä¢', 'üíõ', '‚ô•', 'üö´', 'üèê', '‚Äú', 'J', 'd', 'üåê', 'T', 'üéâ', 'üáØ', 'a', '#', '‚ù§', 'i', 'ü§î'] \n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus_val, vocab_val = words_list(text_val)\n",
    "corpus_train, vocab_train = words_list(text_train)\n",
    "corpus_test, vocab_test = words_list(text_test)\n",
    "\n",
    "print('Vocabulary for text_val.txt:\\n', vocab_val[:20],'\\n')\n",
    "print('Vocabulary for text_train.txt:\\n', vocab_train[:20],'\\n')\n",
    "print('Vocabulary for text_test.txt:\\n', vocab_test[:20],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Cleaning: Pre-processing text file\n",
    "\n",
    "_\n",
    "1. remove puncuations \n",
    "2. convert all words into lower case\n",
    "2. remove stop words\n",
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations from txt files\n",
    "_ meaning signs, spacing and other disturbing features. Alle words are then turned into lower cases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val_text.txt:\n ['user', 'user', 'wiiu', 'is', 'not', 'even', 'a', 'real', 'console', 'user'] \n\n\nval_train.txt:\n ['user', 'bono', 'who', 'cares', 'soon', 'people', 'will', 'understand', 'that', 'they'] \n\n\nval_test.txt:\n ['ibelieveblaseyford', 'is', 'liar', 'she', 'is', 'fat', 'ugly', 'libreal', 'snowflake', 'she'] \n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_val_words = del_punctuations(words_val_txt)\n",
    "cleaned_train_words = del_punctuations(words_train_txt)\n",
    "cleaned_test_words = del_punctuations(words_test_txt)\n",
    "\n",
    "# preview\n",
    "print('val_text.txt:\\n',cleaned_val_words[:10],'\\n')\n",
    "print('\\nval_train.txt:\\n',cleaned_train_words[:10], '\\n')\n",
    "print('\\nval_test.txt:\\n',cleaned_test_words[:10],'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and total number of each variable after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words after removing Stop Words: 17155\nNumber of words after removing Stop Words: 147302\nNumber of words after removing Stop Words: 11080\n"
     ]
    }
   ],
   "source": [
    "cleaned_val_words = stop_words(token_val)\n",
    "cleaned_train_words = stop_words(token_train)\n",
    "cleaned_test_words = stop_words(token_test)\n",
    "\n",
    "# display\n",
    "print(f'Number of words after removing Stop Words: {len(cleaned_val_words)}')\n",
    "print(f'Number of words after removing Stop Words: {len(cleaned_train_words)}')\n",
    "print(f'Number of words after removing Stop Words: {len(cleaned_test_words)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for calculating:\n",
    "\n",
    "1. Number of tokens\n",
    "2. Number of sentences\n",
    "3. Average tokens pr sentence\n",
    "4. Number of unique tokens\n",
    "5. Final number of tokens after removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get number of: \n",
    "# total tokens\n",
    "# total sentences\n",
    "# average tokens\n",
    "# unique tokens \n",
    "# final number of tokens after removing stop words\n",
    "\n",
    "tokens_count, sents_count, avg_tokens, uni_tokens, token_rat, final_tokens = basic_statistics(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of tokens is: 39409\nThe number of sentences is: 2124\nThe average number of tokens per sentence is: 14\nThe number of unique tokens are: 4961\nThe tokens ratio is: 0.126\nThe number of total tokens after removing stopwords are: 17155\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of tokens is: {tokens_count}')\n",
    "print(f'The number of sentences is: {sents_count}')\n",
    "print(f'The average number of tokens per sentence is: {avg_tokens}')\n",
    "print(f'The number of unique tokens are: {uni_tokens}')\n",
    "print(f'The tokens ratio is: {token_rat}')\n",
    "print(f'The number of total tokens after removing stopwords are: {final_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of: \n",
    "# total tokens\n",
    "# total sentences\n",
    "# average tokens\n",
    "# unique tokens \n",
    "# final number of tokens after removing stop words\n",
    "\n",
    "tokens_train, sents_train, avg_train, uni_train,token_train_rat, final_train = basic_statistics(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of tokens is: 336416\nThe number of sentences is: 18122\nThe average number of tokens per sentence is: 14\nThe number of unique tokens are: 17111\nThe tokens ratio is: 0.051\nThe number of total tokens after removing stopwords are: 147302\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of tokens is: {tokens_train}')\n",
    "print(f'The number of sentences is: {sents_train}')\n",
    "print(f'The average number of tokens per sentence is: {avg_train}')\n",
    "print(f'The number of unique tokens are: {uni_train}')\n",
    "print(f'The tokens ratio is: {token_train_rat}')\n",
    "print(f'The number of total tokens after removing stopwords are: { final_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of: \n",
    "# total tokens\n",
    "# total sentences\n",
    "# average tokens\n",
    "# unique tokens \n",
    "# final number of tokens after removing stop words\n",
    "\n",
    "tokens_test, sents_test, avg_test, uni_test, token_test_rat, final_test = basic_statistics(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of tokens is: 26036\nThe number of sentences is: 1214\nThe average number of tokens per sentence is: 16\nThe number of unique tokens are: 4839\nThe tokens ratio is: 0.186\nThe number of total tokens after removing stopwords are: 11080\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of tokens is: {tokens_test}')\n",
    "print(f'The number of sentences is: {sents_test}')\n",
    "print(f'The average number of tokens per sentence is: {avg_test}')\n",
    "print(f'The number of unique tokens are: {uni_test}')\n",
    "print(f'The tokens ratio is: {token_test_rat}')\n",
    "print(f'The number of total tokens after removing stopwords are: { final_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in val_text:\n",
      " [('love', 31), ('great', 31), ('much', 32), ('democrats', 32), ('country', 32), ('could', 33), ('vote', 33), ('shit', 33), ('never', 35), ('believe', 35), ('way', 35), ('need', 36), ('say', 37), ('still', 38), ('time', 40), ('make', 40), ('go', 41), ('good', 42), ('even', 43), ('see', 44), ('right', 45), ('going', 47), ('us', 52), ('want', 55), ('would', 61), ('amp', 62), ('think', 68), ('trump', 69), ('one', 71), ('get', 73), ('know', 77), ('people', 89), ('maga', 98), ('conservatives', 107), ('like', 109), ('antifa', 118), ('control', 125), ('gun', 133), ('liberals', 137)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in train_text:\n",
      " [('better', 237), ('vote', 237), ('well', 240), ('much', 249), ('left', 252), ('still', 262), ('make', 276), ('way', 278), ('really', 284), ('love', 290), ('back', 290), ('say', 292), ('even', 294), ('going', 312), ('see', 318), ('shit', 319), ('never', 325), ('need', 326), ('want', 329), ('go', 340), ('us', 345), ('time', 349), ('right', 409), ('good', 416), ('think', 483), ('would', 507), ('know', 557), ('trump', 565), ('one', 568), ('get', 586), ('amp', 615), ('people', 830), ('conservatives', 839), ('maga', 907), ('like', 995), ('antifa', 1047), ('control', 1095), ('gun', 1230), ('liberals', 1260)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in test_text:\n",
      " [('chicago', 18), ('keep', 19), ('even', 19), ('years', 21), ('go', 21), ('really', 21), ('think', 22), ('life', 22), ('still', 22), ('always', 23), ('support', 24), ('shit', 24), ('time', 25), ('please', 25), ('way', 26), ('democrats', 26), ('need', 27), ('kavanaugh', 28), ('never', 28), ('see', 29), ('going', 30), ('new', 30), ('know', 31), ('good', 31), ('via', 33), ('want', 37), ('love', 38), ('us', 42), ('trump', 44), ('people', 47), ('one', 48), ('get', 51), ('maga', 57), ('gun', 64), ('control', 64), ('like', 65), ('antifa', 74), ('conservatives', 80), ('liberals', 81)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq_words_val = word_frequency(cleaned_val_words)\n",
    "freq_words_train = word_frequency(cleaned_train_words)\n",
    "freq_words_test = word_frequency(cleaned_test_words)\n",
    "\n",
    "# displays the final 40 elements from the back end of the list\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in val_text:\\n',list(freq_words_val.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in train_text:\\n',list(freq_words_train.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in test_text:\\n',list(freq_words_test.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 20 in val_text.txt:\n {'user': 3455, 'liberals': 137, 'gun': 133, 'control': 125, 'antifa': 118, 'like': 109, 'conservatives': 107, 'maga': 98, 'people': 89, 'know': 77, 'get': 73, 'one': 71, 'trump': 69, 'think': 68, 'amp': 62, 'would': 61, 'want': 55, 'us': 52, 'going': 47, 'right': 45} \n\nTop 20 in val_train.txt:\n {'user': 29961, 'liberals': 1260, 'gun': 1230, 'control': 1095, 'antifa': 1047, 'like': 995, 'maga': 907, 'conservatives': 839, 'people': 830, 'amp': 615, 'get': 586, 'one': 568, 'trump': 565, 'know': 557, 'would': 507, 'think': 483, 'good': 416, 'right': 409, 'time': 349, 'us': 345} \n\nTop 20 in val_test.txt:\n {'user': 608, 'liberals': 81, 'conservatives': 80, 'antifa': 74, 'like': 65, 'gun': 64, 'control': 64, 'maga': 57, 'get': 51, 'one': 48, 'people': 47, 'trump': 44, 'us': 42, 'love': 38, 'want': 37, 'via': 33, 'know': 31, 'good': 31, 'going': 30, 'new': 30} \n\n"
     ]
    }
   ],
   "source": [
    "# top 20 of common words\n",
    "top_20_val = top_20_most_common_words(freq_words_val)\n",
    "top_20_train = top_20_most_common_words(freq_words_train)\n",
    "top_20_test = top_20_most_common_words(freq_words_test)\n",
    "\n",
    "print('Top 20 in val_text.txt:\\n',top_20_val, '\\n')\n",
    "print('Top 20 in val_train.txt:\\n',top_20_train,'\\n')\n",
    "print('Top 20 in val_test.txt:\\n',top_20_test,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_source:https://medium.com/vickdata/detecting-hate-speech-in-tweets-natural-language-processing-in-python-for-beginners-4e591952223 _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Set: (11916, 1) 11916\nTest Set: (860, 1) 860\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('datasets/offensive/train_text.txt', header=None, skiprows = 0, names=['tweet'], sep='\\t', quoting=3)\n",
    "print(\"Training Set:\"% train.columns, train.shape, len(train))\n",
    "\n",
    "\n",
    "test = pd.read_csv('datasets/offensive/test_text.txt',header=None, skiprows = 0, names=['tweet'], sep='\\t', quoting=3)\n",
    "print(\"Test Set:\"% test.columns, test.shape, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_train = [x for x in range(1, len(train.values)+1)]\n",
    "index_test = [x for x in range(1, len(test.values)+1)]\n",
    "\n",
    "train.insert(loc=0, column='id', value =index_train )\n",
    "test.insert(loc=0, column='id', value =index_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_labels = pd.read_csv('datasets/offensive/train_labels.txt',header=None, skiprows = 0, names=['label'], sep='\\t', quoting=3)\n",
    "train_labels.insert(loc=0, column='id', value=index_train)\n",
    "\n",
    "test_labels = pd.read_csv('datasets/offensive/test_labels.txt',header=None, skiprows = 0, names=['label'], sep='\\t', quoting=3)\n",
    "test_labels.insert(loc=0, column='id', value =index_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = test.merge(test_labels, on='id', how='left')\n",
    "train_df = train.merge(train_labels, on='id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id                                              tweet  label\n",
       "0          1  @user Bono... who cares. Soon people will unde...      0\n",
       "1          2  @user Eight years the republicans denied obama...      1\n",
       "2          3  @user Get him some line help. He is gonna be j...      0\n",
       "3          4               @user @user She is great. Hi Fiona!       0\n",
       "4          5  @user She has become a parody unto herself? Sh...      1\n",
       "...      ...                                                ...    ...\n",
       "11911  11912   @user I wonder if they are sex traffic victims?       1\n",
       "11912  11913  @user Do we dare say he is... better than Nyjer?       0\n",
       "11913  11914                    @user No idea who he is. Sorry       0\n",
       "11914  11915  #Professor Who Shot Self Over Trump Says Gun C...      0\n",
       "11915  11916  @user @user @user Here your proof!  Our Africa...      1\n",
       "\n",
       "[11916 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>@user Bono... who cares. Soon people will unde...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>@user Eight years the republicans denied obama...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>@user Get him some line help. He is gonna be j...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>@user @user She is great. Hi Fiona!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>@user She has become a parody unto herself? Sh...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11911</th>\n      <td>11912</td>\n      <td>@user I wonder if they are sex traffic victims?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11912</th>\n      <td>11913</td>\n      <td>@user Do we dare say he is... better than Nyjer?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11913</th>\n      <td>11914</td>\n      <td>@user No idea who he is. Sorry</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11914</th>\n      <td>11915</td>\n      <td>#Professor Who Shot Self Over Trump Says Gun C...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11915</th>\n      <td>11916</td>\n      <td>@user @user @user Here your proof!  Our Africa...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>11916 rows √ó 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id                                              tweet  label\n",
       "0      1  #ibelieveblaseyford is liar she is fat ugly li...      1\n",
       "1      2  @user @user @user I got in a pretty deep debat...      0\n",
       "2      3  ...if you want more shootings and more death, ...      0\n",
       "3      4  Angels now have 6 runs. Five of them have come...      0\n",
       "4      5  #Travel #Movies and Unix #Fortune combined  Vi...      0\n",
       "..   ...                                                ...    ...\n",
       "855  856  #CNN irrationally argues 4 legalising #abortio...      0\n",
       "856  857  @user @user @user @user @user @user @user @use...      0\n",
       "857  858  #Conservatives don‚Äôt care what you post..it‚Äôs ...      1\n",
       "858  859  #antifa #Resist.. Trump is trying to bring wor...      0\n",
       "859  860  #Maine you need to face facts @user doesn‚Äôt re...      0\n",
       "\n",
       "[860 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>#ibelieveblaseyford is liar she is fat ugly li...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>@user @user @user I got in a pretty deep debat...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>...if you want more shootings and more death, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Angels now have 6 runs. Five of them have come...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>#Travel #Movies and Unix #Fortune combined  Vi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>855</th>\n      <td>856</td>\n      <td>#CNN irrationally argues 4 legalising #abortio...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>857</td>\n      <td>@user @user @user @user @user @user @user @use...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>858</td>\n      <td>#Conservatives don‚Äôt care what you post..it‚Äôs ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>858</th>\n      <td>859</td>\n      <td>#antifa #Resist.. Trump is trying to bring wor...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>859</th>\n      <td>860</td>\n      <td>#Maine you need to face facts @user doesn‚Äôt re...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>860 rows √ó 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = clean_text(test_df, 'tweet')\n",
    "train_clean = clean_text(train_df, 'tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1   ibelieveblaseyford is liar she is fat ugly li...      1\n",
       "1   2        i got in a pretty deep debate with my fr...      0\n",
       "2   3     if you want more shootings and more death  ...      0\n",
       "3   4  angels now have 6 runs  five of them have come...      0\n",
       "4   5   travel  movies and unix  fortune combined  vi...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ibelieveblaseyford is liar she is fat ugly li...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>i got in a pretty deep debate with my fr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>if you want more shootings and more death  ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>angels now have 6 runs  five of them have come...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>travel  movies and unix  fortune combined  vi...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1  bono cares soon people understand gain nothing...      0\n",
       "1   2  eight years republicans denied obama picks bre...      1\n",
       "2   3  get line help gonna fine game went could see p...      0\n",
       "3   4                                     great hi fiona      0\n",
       "4   5  become parody unto certainly taken heat well i...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>bono cares soon people understand gain nothing...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>eight years republicans denied obama picks bre...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>get line help gonna fine game went could see p...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>great hi fiona</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>become parody unto certainly taken heat well i...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "train_clean['tweet'] = train_clean['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1  ibelieveblaseyford liar fat ugly libreal snowf...      1\n",
       "1   2  got pretty deep debate friend told latinos tru...      0\n",
       "2   3  want shootings death listen aclu black lives m...      0\n",
       "3   4  angels 6 runs five come courtesy mike trout ho...      0\n",
       "4   5  travel movies unix fortune combined visit sali...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ibelieveblaseyford liar fat ugly libreal snowf...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>got pretty deep debate friend told latinos tru...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>want shootings death listen aclu black lives m...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>angels 6 runs five come courtesy mike trout ho...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>travel movies unix fortune combined visit sali...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "test_clean['tweet'] = test_clean['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1  [ibelieveblaseyford, liar, fat, ugly, libreal,...      1\n",
       "1   2  [got, pretty, deep, debate, friend, told, lati...      0\n",
       "2   3  [want, shootings, death, listen, aclu, black, ...      0\n",
       "3   4  [angels, 6, runs, five, come, courtesy, mike, ...      0\n",
       "4   5  [travel, movies, unix, fortune, combined, visi...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[ibelieveblaseyford, liar, fat, ugly, libreal,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[got, pretty, deep, debate, friend, told, lati...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[want, shootings, death, listen, aclu, black, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[angels, 6, runs, five, come, courtesy, mike, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[travel, movies, unix, fortune, combined, visi...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "test_clean['tweet'] = test_clean['tweet'].apply(lambda x: word_tokenize(x))\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    id                                              tweet  label\n",
       "0    1  [ibelieveblaseyford, liar, fat, ugly, libreal,...      1\n",
       "8    9  [grown, ass, woman, probably, 10, years, older...      1\n",
       "10  11  [kavanaugh, disciple, anthony, kennedy, ed, li...      1\n",
       "15  16  [apparently, committed, going, new, level, sin...      1\n",
       "18  19  [50, cent, calls, joe, budden, bullshit, insta...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[ibelieveblaseyford, liar, fat, ugly, libreal,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>[grown, ass, woman, probably, 10, years, older...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>[kavanaugh, disciple, anthony, kennedy, ed, li...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>[apparently, committed, going, new, level, sin...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>[50, cent, calls, joe, budden, bullshit, insta...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "test_clean_offensive = test_clean.loc[test_clean['label'] == 1]\n",
    "\n",
    "#est_clean_offensive['tweet'].to_csv('test_cleaned_off_lang_df.csv',  quoting=csv.QUOTE_NONE, escapechar=' ') \n",
    "test_clean_offensive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "1   2  [got, pretty, deep, debate, friend, told, lati...      0\n",
       "2   3  [want, shootings, death, listen, aclu, black, ...      0\n",
       "3   4  [angels, 6, runs, five, come, courtesy, mike, ...      0\n",
       "4   5  [travel, movies, unix, fortune, combined, visi...      0\n",
       "5   6  [naturephotography, nature, birds, wild, wisco...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[got, pretty, deep, debate, friend, told, lati...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[want, shootings, death, listen, aclu, black, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[angels, 6, runs, five, come, courtesy, mike, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[travel, movies, unix, fortune, combined, visi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>[naturephotography, nature, birds, wild, wisco...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "test_clean_nonoffensive = test_clean.loc[test_clean['label'] == 0]\n",
    "\n",
    "#test_clean_nonoffensive['tweet'].to_csv('test_cleaned_non_off_lang_df.csv',  quoting=csv.QUOTE_NONE, escapechar=' ') \n",
    "test_clean_nonoffensive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = test_clean_nonoffensive.explode('tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test_clean_offensive['tweet'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of words on non-cleaned texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bag_test = bag_of_words(text_test)\n",
    "bag_train = bag_of_words(text_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq_test = freq_words(bag_test)\n",
    "freq_train = freq_words(bag_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'üß°'"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "max(bag_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tok = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "def make_corpus(filename):\n",
    "    corpus = []\n",
    "    with open(filename, 'r',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            corpus.extend(t for line in f for t in tok.tokenize(line))\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus_train = make_corpus('datasets/offensive/train_text.txt')\n",
    "corpus_value = make_corpus('datasets/offensive/val_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataframe_corpus(corpus):\n",
    "    voc = collections.Counter(corpus)\n",
    "    frq = pd.DataFrame(voc.most_common(), columns=['token', 'frequency'])\n",
    "\n",
    "    return frq\n",
    "\n",
    "def freq_cum(frq, corpus):\n",
    "    # Index in the sorted list\n",
    "    frq['idx'] = frq.index + 1\n",
    "\n",
    "    # Frequency normalised by corpus size\n",
    "    frq['norm_freq'] = frq.frequency / len(corpus)\n",
    "\n",
    "    # Cumulative normalised frequency\n",
    "    frq['cumul_frq'] = frq.norm_freq.cumsum()\n",
    "\n",
    "    return frq\n",
    "\n",
    "\n",
    "frq_train = dataframe_corpus(corpus_train)\n",
    "frq_train.head()\n",
    "frq_train = freq_cum(frq_train, corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "frq_value = dataframe_corpus(corpus_value)\n",
    "frq_value = freq_cum(frq_value, corpus_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  token  frequency  idx  norm_freq  cumul_frq\n",
       "0     @      29977    1   0.091424   0.091424\n",
       "1  user      29811    2   0.090917   0.182341\n",
       "2   the       7249    3   0.022108   0.204449\n",
       "3    is       5643    4   0.017210   0.221659\n",
       "4    to       5454    5   0.016634   0.238293"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>frequency</th>\n      <th>idx</th>\n      <th>norm_freq</th>\n      <th>cumul_frq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@</td>\n      <td>29977</td>\n      <td>1</td>\n      <td>0.091424</td>\n      <td>0.091424</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>user</td>\n      <td>29811</td>\n      <td>2</td>\n      <td>0.090917</td>\n      <td>0.182341</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the</td>\n      <td>7249</td>\n      <td>3</td>\n      <td>0.022108</td>\n      <td>0.204449</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>is</td>\n      <td>5643</td>\n      <td>4</td>\n      <td>0.017210</td>\n      <td>0.221659</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>to</td>\n      <td>5454</td>\n      <td>5</td>\n      <td>0.016634</td>\n      <td>0.238293</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "frq_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  token  frequency  idx  norm_freq  cumul_frq\n",
       "0     @       3457    1   0.090073   0.090073\n",
       "1  user       3429    2   0.089343   0.179416\n",
       "2   the        822    3   0.021417   0.200834\n",
       "3    to        706    4   0.018395   0.219229\n",
       "4    is        665    5   0.017327   0.236555"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>frequency</th>\n      <th>idx</th>\n      <th>norm_freq</th>\n      <th>cumul_frq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@</td>\n      <td>3457</td>\n      <td>1</td>\n      <td>0.090073</td>\n      <td>0.090073</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>user</td>\n      <td>3429</td>\n      <td>2</td>\n      <td>0.089343</td>\n      <td>0.179416</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the</td>\n      <td>822</td>\n      <td>3</td>\n      <td>0.021417</td>\n      <td>0.200834</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>to</td>\n      <td>706</td>\n      <td>4</td>\n      <td>0.018395</td>\n      <td>0.219229</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>is</td>\n      <td>665</td>\n      <td>5</td>\n      <td>0.017327</td>\n      <td>0.236555</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "frq_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set(style='whitegrid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot: Cumulative frequency by index\n",
    "def freq_cum_plot(frq):\n",
    "    seaborn.relplot(x='idx', y='cumul_frq', data=frq)\n",
    "    return plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_cum_plot(frq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_cum_plot(frq_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Cumulative frequency by index, top x tokens\n",
    "def freq_cum_plot_top_x_tokens(frq, top_x):\n",
    "    seaborn.relplot(x='idx', y='cumul_frq', data=frq[:int(top_x)], kind='line')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_cum_plot_top_x_tokens(frq_train, 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_cum_plot_top_x_tokens(frq_value, 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Log-log plot for Zipf's law\n",
    "\n",
    "def zipfs_law(frq):\n",
    "    frq['log_frq'] = np.log(frq.frequency)\n",
    "    frq['log_rank'] = np.log(frq.frequency.rank(ascending=True))\n",
    "    seaborn.relplot(x='log_rank', y='log_frq', data=frq)\n",
    "    return plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zipfs_law(frq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zipfs_law(frq_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "_each sentence is classified into predined categories with the purpose to understand the content of the sentences via extracting key information._\n",
    "\n",
    "\n",
    "Entity types:\n",
    "\n",
    "Names of people,\n",
    "\n",
    "Organizations\n",
    "\n",
    "Locations\n",
    "\n",
    "Times\n",
    "\n",
    "Quantities\n",
    "\n",
    "Monetary values\n",
    "\n",
    "Percentages\n",
    "and more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['#ibelieveblaseyford', 'is', 'liar', 'she', 'is']\n['#', 'ibelieveblaseyford', 'is', 'liar', 'she']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import  word_tokenize\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "tweet_tokenizer = tt.tokenize(text_test)\n",
    "word_tweet_tok = word_tokenize(text_test)\n",
    "\n",
    "print(tweet_tokenizer[:5])\n",
    "print(word_tweet_tok[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If an error arrises type \"python -m spacy download en_core_web_\" into your anaconda terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the text with spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'doc' now contains a parsed version of text file. We will use 'doc' to print named entities that were detected in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_,\n",
    "    token.is_alpha, token.is_stop,\n",
    "        [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dframcy import DframCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframcy = DframCy(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = dframcy.nlp(text)\n",
    "annotation_dataframe = dframcy.to_dataframe(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotation_dataframe.to_csv('annotation_df_test_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4761 74788 ORG\n",
      "#t4us # 74934 74941 MONEY\n",
      "#you # 75342 75348 MONEY\n",
      "#to # 75364 75369 MONEY\n",
      "11.09.18 75423 75431 CARDINAL\n",
      "Instagram 75487 75496 ORG\n",
      "week-long 75554 75563 DATE\n",
      "Linn County 75573 75584 GPE\n",
      "Gregory Davis 75600 75613 PERSON\n",
      "First-Degree Murder 75617 75636 ORG\n",
      "Carrie Davis 75660 75672 PERSON\n",
      "almost a year ago 75698 75715 DATE\n",
      "Serena 75734 75740 PERSON\n",
      "Eight 76042 76047 CARDINAL\n",
      "about #Abortion Pill 76056 76076 MONEY\n",
      "Works for Company that Produces 76083 76114 ORG\n",
      "# 76121 76122 CARDINAL\n",
      "CorceptTherapeutics Inc. 76122 76146 ORG\n",
      "CORT 76149 76153 GPE\n",
      "#RU486 # 76154 76162 MONEY\n",
      "Kavanaugh 76173 76182 ORG\n",
      "#CORT #RoevWade #KavanaughConfirmation  @user 76185 76230 MONEY\n",
      "10 years ago 76238 76250 DATE\n",
      "Taylor Swift 76258 76270 PERSON\n",
      "first 76348 76353 ORDINAL\n",
      "#10YearsOfLoveStory \n",
      " 76369 76390 MONEY\n",
      "California 76391 76401 GPE\n",
      "8 76410 76411 CARDINAL\n",
      "the United States 76639 76656 GPE\n",
      "Saudi Arabian 76702 76715 NORP\n",
      "AMC 76879 76882 ORG\n",
      "üéâ 77097 77098 PERSON\n",
      "üíô \n",
      "@user 77098 77106 PERSON\n",
      "Democrats 77181 77190 NORP\n",
      "America 77262 77269 GPE\n",
      "Dov Hikind 77295 77305 PERSON\n",
      "Democratic 77307 77317 NORP\n",
      "American 77337 77345 NORP\n",
      "Julieanna Godard 77538 77554 PERSON\n",
      "NowPlaying 77821 77831 ORG\n",
      "Sheard - You Are on \n",
      "#Learn 77846 77873 WORK_OF_ART\n",
      "# 77969 77970 DATE\n",
      "#AboutThatLife #CLifeMacon \n",
      "Ai 77982 78012 MONEY\n",
      "Death Cab For Cutie / 78076 78097 WORK_OF_ART\n",
      "Alex Jones 78144 78154 PERSON\n",
      "I‚Äôm Canadian 78302 78314 PERSON\n",
      "Osaka 78345 78350 GPE\n",
      "HealingYourself 78509 78524 ORG\n",
      "2,000 78561 78566 MONEY\n",
      "Josh McConnell 78596 78610 PERSON\n",
      "Apple 78656 78661 ORG\n",
      "HurricaneFlorerence 78815 78834 ORG\n",
      "Democrats 78850 78859 NORP\n",
      "2 78861 78862 CARDINAL\n",
      "Democrats 78907 78916 NORP\n",
      "Ellison 78942 78949 PERSON\n",
      "Spartacus 79029 79038 PERSON\n",
      "Booker 79040 79046 PERSON\n",
      "1992 79098 79102 DATE\n",
      "FemiNazis 79131 79140 ORG\n",
      "Liberals 79145 79153 NORP\n",
      "KAG2018 79222 79229 GPE\n",
      "India 79326 79331 GPE\n",
      "Muslim 79382 79388 NORP\n",
      "Muslim 79399 79405 NORP\n",
      "#WFMMC # 79556 79564 MONEY\n",
      "#Sripuptruzman \n",
      " 79577 79593 MONEY\n",
      "calgary 79677 79684 GPE\n",
      "Rosie 79739 79744 PERSON\n",
      "# 79778 79779 DATE\n",
      "6 80047 80048 CARDINAL\n",
      "#ohreally \n",
      "# 80220 80232 MONEY\n",
      "Bakersfield 80232 80243 GPE\n",
      "BlaseyFord 80372 80382 MONEY\n",
      "#LiberalismIsAMentalDisorder \n",
      "# 80470 80501 MONEY\n",
      "gayagenda 80825 80834 ORG\n",
      "republicans 80840 80851 NORP\n",
      "#zionistagenda \n",
      " 80866 80882 MONEY\n",
      "GOP 80907 80910 ORG\n",
      "#QAnon 80937 80943 MONEY\n",
      "#DAVOS # 80976 80984 MONEY\n",
      "California 80984 80994 GPE\n",
      "FOXNEWS 80996 81003 ORG\n",
      "# 81004 81005 DATE\n",
      "CNN 81005 81008 ORG\n",
      "#WSJ 81009 81013 MONEY\n",
      "NYSE 81015 81019 ORG\n",
      "WAKEUPAMERICA 81021 81034 ORG\n",
      "MAGA 81036 81040 ORG\n",
      "1/2 81076 81079 CARDINAL\n",
      "1stAmendment 81309 81321 MONEY\n",
      "NJ 81468 81470 GPE\n",
      "Christian 81596 81605 NORP\n",
      "Twitter 81609 81616 PERSON\n",
      "Facebook 81621 81629 ORG\n",
      "Christians 81657 81667 NORP\n",
      "0.20 81992 81996 MONEY\n",
      "Americans 82122 82131 NORP\n",
      "tomorrow 82291 82299 DATE\n",
      "craig 82327 82332 PERSON\n",
      "üëá  EEA/EFTA 82522 82533 PERSON\n",
      "Norway 82535 82541 GPE\n",
      "CETA+++ 82566 82573 GPE\n",
      "Canada 82575 82581 GPE\n",
      "bb20 82622 82626 ORG\n",
      "Jc 82664 82666 PERSON\n",
      "Angela 82684 82690 PERSON\n",
      "bb20  # 82875 82882 PERSON\n",
      "ACLU 82948 82952 ORG\n",
      "Antifa 82954 82960 NORP\n",
      "BroncosVsSeahawks #BroncosCountry Crue 83052 83090 ORG\n",
      "NE 83425 83427 ORG\n",
      "first 83646 83651 ORDINAL\n",
      "Canada 83672 83678 GPE\n",
      "CambridgeAnalytica 83797 83815 ORG\n",
      "BecauseOfTories 83826 83841 ORG\n",
      "Noname 83937 83943 PERSON\n",
      "like 5 days 83996 84007 DATE\n",
      "Melbourne 84011 84020 GPE\n",
      "a nice day 84041 84051 DATE\n",
      "ChristineBlaseyFord 84131 84150 MONEY\n",
      "#ConfirmJudgeKavanaugh \n",
      "#GreatAwakening # 84214 84255 MONEY\n",
      "üò° \n",
      " 84515 84518 MONEY\n",
      "Latino 84541 84547 NORP\n",
      "Trump 84577 84582 ORG\n",
      "Christian 84586 84595 NORP\n",
      "Latinos 84652 84659 NORP\n",
      "the Democratic Party 84772 84792 ORG\n",
      "Clinton 84816 84823 PERSON\n",
      "Monsanto 84825 84833 ORG\n",
      "#BigPharma # 84842 84854 MONEY\n",
      "# 84862 84863 DATE\n",
      "Hillary 85036 85043 PERSON\n",
      "100DaysofCode 85076 85089 MONEY\n",
      "Montreal 85126 85134 GPE\n",
      "Antonia Evans 85136 85149 PERSON\n",
      "Altleft 85231 85238 ORG\n",
      "Law 85245 85248 PERSON\n",
      "#Conservatives 85323 85337 MONEY\n",
      "UPenn 85371 85376 GPE\n",
      "AltLeft 85378 85385 ORG\n",
      "ABA 85397 85400 ORG\n",
      "#HigherEducation # 85401 85419 MONEY\n",
      "Kavanaugh  John Fund 85437 85457 ORG\n",
      "Kavanaugh 85459 85468 ORG\n",
      "Ford 85473 85477 ORG\n",
      "1991 85497 85501 DATE\n",
      "Spaldo 85561 85567 PERSON\n",
      "Lovetoread Share 85682 85698 PERSON\n",
      "Wednesday 85734 85743 DATE\n",
      "US 85827 85829 GPE\n",
      "#HousingCrisis  \n",
      " 86138 86155 MONEY\n",
      "TEXAS 86156 86161 GPE\n",
      "Socialist 86174 86183 ORG\n",
      "TEXAS 86241 86246 GPE\n",
      "American 86305 86313 NORP\n",
      "TEXAS 86344 86349 GPE\n",
      "SaveTexas 86423 86432 ORG\n",
      "Arizona 86435 86442 GPE\n",
      "Tennessee 86444 86453 GPE\n",
      "Alaska 86456 86462 GPE\n",
      "SC 86464 86466 GPE\n",
      "EU 86586 86588 ORG\n",
      "U.S 86683 86686 GPE\n",
      "Trump 86708 86713 ORG\n",
      "#maga \n",
      " 86862 86869 MONEY\n",
      "NYFW 86896 86900 ORG\n",
      "Donald J Trump 86999 87013 PERSON\n",
      "Soros 87521 87526 PERSON\n",
      "Kids Beating 87575 87587 WORK_OF_ART\n",
      "Left Teach Their #Children Hate Violence 87615 87655 WORK_OF_ART\n",
      "Democrats 87678 87687 NORP\n",
      "# 87688 87689 DATE\n",
      "WalkAway 87699 87707 ORG\n",
      "Christian 87744 87753 NORP\n",
      "BlackTwitter 87805 87817 ORG\n",
      "#MAGA \n",
      " 87849 87856 MONEY\n",
      "Beto O'Rourke 87857 87870 PERSON\n",
      "Houston 87892 87899 GPE\n",
      "CALIFORNIA 87928 87938 GPE\n",
      "PATRIOTS 87939 87947 ORG\n",
      "Soros 87984 87989 PERSON\n",
      "House 88018 88023 ORG\n",
      "Congress 88047 88055 ORG\n",
      "TJ Cox 88068 88074 PERSON\n",
      "Candidate 88091 88100 ORG\n",
      "David Valadao 88156 88169 PERSON\n",
      "House 88174 88179 ORG\n",
      "Dist 21 88181 88188 ORG\n",
      "CoryBooker 88205 88215 ORG\n",
      "American 88240 88248 NORP\n",
      "KavanaughConfirmation 88333 88354 ORG\n",
      "Hmmn 88383 88387 PERSON\n",
      "YouthCuts 88454 88463 GPE\n",
      "# 88464 88465 DATE\n",
      "StraightRazors 88465 88479 ORG\n",
      "üèø 88487 88488 PERSON\n",
      "ü§µ 88488 88489 DATE\n",
      "üèø 88491 88492 PERSON\n",
      "#7sBarbershop #SouthOrangeVillage \n",
      " 88508 88543 MONEY\n",
      "Beyonce 88577 88584 PERSON\n",
      "Ahmed 88685 88690 PERSON\n",
      "Sohail sahab 88697 88709 PERSON\n",
      "PMJG 88764 88768 ORG\n",
      "#JPNA2 Humayun 88773 88787 MONEY\n",
      "Shaan 88795 88800 PERSON\n",
      "Fahad 88806 88811 PERSON\n",
      "pakistan 88833 88841 GPE\n",
      "Conservatives 89132 89145 NORP\n",
      "Labour 89195 89201 ORG\n",
      "36 years ago 89296 89308 DATE\n",
      "A Rape A Violent Sexual Event by Move 89337 89374 WORK_OF_ART\n",
      "Soros 89377 89382 PERSON\n",
      "BLM Antifa 89408 89418 PRODUCT\n",
      "1972 89472 89476 DATE\n",
      "AUSTRALIA 89550 89559 GPE\n",
      "NEW ZEALAND 89564 89575 GPE\n",
      "#antifa # 89595 89604 MONEY\n",
      "#FCKNZS \n",
      " 89611 89620 MONEY\n",
      "MAXDRIVETIMETRAVELTHURSDAY 89641 89667 PERSON\n",
      "NP Home 89693 89700 ORG\n",
      "MAGA 89828 89832 ORG\n",
      "#Kavanaugh #DrainTheDeepState @user 89833 89868 MONEY\n",
      "ALL OUT WAR 89924 89935 WORK_OF_ART\n",
      "today 89972 89977 DATE\n",
      "DC Secret Resistance Society 89980 90008 ORG\n",
      "White House 90021 90032 ORG\n",
      "2 yrs 90069 90074 DATE\n",
      "Bk 90081 90083 ORG\n",
      "#Democrats \n",
      "# 90306 90319 MONEY\n",
      "The day 90452 90459 DATE\n",
      "Dowd 90494 90498 PERSON\n",
      "ClimatChange 90521 90533 ORG\n",
      "MendesArmy 90786 90796 PRODUCT\n",
      "millions 90844 90852 CARDINAL\n",
      "Antifa Terrorist Organizations 90968 90998 ORG\n",
      "Customer Service 91040 91056 ORG\n",
      "HurricaneFlorence 91239 91256 ORG\n",
      "this week 91257 91266 DATE\n",
      "Delaware 91353 91361 GPE\n",
      "the day 91390 91397 DATE\n",
      "Shia 91448 91452 NORP\n",
      "Sunni 91456 91461 NORP\n",
      "WCE 91512 91515 ORG\n",
      "Canada 91713 91719 GPE\n",
      "#Trudeau # 91733 91743 MONEY\n",
      "$372M 91758 91763 MONEY\n",
      "5 91816 91817 CARDINAL\n",
      "FED Agency - Hijaked 91819 91839 ORG\n",
      "FED Agency 91853 91863 ORG\n",
      "BLM 91908 91911 ORG\n",
      "Soros 91912 91917 PERSON\n",
      "CIA 91918 91921 ORG\n",
      "Conservatives 91924 91937 NORP\n",
      "Bible 92040 92045 WORK_OF_ART\n",
      "Republicans 92101 92112 NORP\n",
      "Kavanaugh 92129 92138 ORG\n",
      "17 year old 92151 92162 DATE\n",
      "MAGA 92196 92200 ORG\n",
      "MAGA 92549 92553 ORG\n",
      "# 92554 92555 DATE\n",
      "#GreatAwakeningWorldwide # 92590 92616 MONEY\n",
      "Antifa 92718 92724 NORP\n",
      "# 92790 92791 DATE\n",
      "#FemiNazi    92807 92819 MONEY\n",
      "WalkAway 92831 92839 ORG\n",
      "100 92846 92849 CARDINAL\n",
      "Kiddin Me 92874 92883 PERSON\n",
      "FauxProgressives 92887 92903 ORG\n",
      "CorruptDemocrats 92931 92947 ORG\n",
      "years 92952 92957 DATE\n",
      "Cuomo 93023 93028 PERSON\n",
      "Cuomo Flips Off 93032 93047 PERSON\n",
      "CoryBooker 93088 93098 ORG\n",
      "109 years 93233 93242 DATE\n",
      "U.S 93243 93246 GPE\n",
      "U.S Citizens Not 93289 93305 ORG\n",
      "U.S 93340 93343 GPE\n",
      "Trump 93346 93351 ORG\n",
      "MAGA 93353 93357 ORG\n",
      "MAGA 93372 93376 ORG\n",
      "WalkAway 93404 93412 ORG\n",
      "DemocratSocialists 93414 93432 ORG\n",
      "#Socialism 93445 93455 PERSON\n",
      "# 93476 93477 DATE\n",
      "Antifa 93477 93483 NORP\n",
      "#Cult 93484 93489 PERSON\n",
      "EmptyHeaded 93521 93532 ORG\n",
      "BernieSanders 93534 93547 ORG\n",
      "Venezuela 93549 93558 GPE\n",
      "#AlexandriaOcasioCortez 93584 93607 MONEY\n",
      "OcasioCortez 93609 93621 ORG\n",
      "#EndorsementFail 93622 93638 MONEY\n",
      "Alexandria Ocasio Cortez \n",
      "@user 93658 93689 PERSON\n",
      "Today 93805 93810 DATE\n",
      "two 93815 93818 CARDINAL\n",
      "Mark Twain 93909 93919 PERSON\n",
      "Erika Ikizake 94042 94055 PERSON\n",
      "7-Day 94066 94071 DATE\n",
      "MARTA 94095 94100 ORG\n",
      "Campus Services! 94105 94121 ORG\n",
      "Staytuned 94158 94167 GPE\n",
      "Cha 94397 94400 PERSON\n",
      "Busta Rhymes 94430 94442 PERSON\n",
      "1000daysofsoloharry 94447 94466 MONEY\n",
      "Democrats 94722 94731 NORP\n",
      "BrettKavanaugh 94754 94768 ORG\n",
      "TedKennedy 94815 94825 ORG\n",
      "Senate 94878 94884 ORG\n",
      "America 95140 95147 GPE\n",
      "today 95148 95153 DATE\n",
      "Zamalek 95164 95171 GPE\n",
      "Zamo!Mapholoba!She 95172 95190 PERSON\n",
      "first 95198 95203 ORDINAL\n",
      "Uni 95214 95217 GPE\n",
      "today 95223 95228 DATE\n",
      "Pholobs 95262 95269 ORG\n",
      "8 years 95271 95278 DATE\n",
      "Brexit 95340 95346 PERSON\n",
      "May 95393 95396 DATE\n",
      "#ForTheMany \n",
      " 95446 95459 MONEY\n",
      "Antifa 95460 95466 NORP\n",
      "Democrats 95763 95772 NORP\n",
      "Trump 95845 95850 ORG\n",
      "Hillary Clinton 95997 96012 PERSON\n",
      "üëã 96163 96164 PRODUCT\n",
      "Marxists 96205 96213 NORP\n",
      "Antifa 96413 96419 NORP\n",
      "Conservatives 96438 96451 NORP\n",
      "Tories 96483 96489 GPE\n",
      "#Tory 96490 96495 PERSON\n",
      "millions 96589 96597 CARDINAL\n",
      "PedoWood 96764 96772 ORG\n",
      "Haiti 96774 96779 GPE\n",
      "3 96911 96912 CARDINAL\n",
      "An Amazing Business Opportunity 96937 96968 ORG\n",
      "Anna Ladd 96972 96981 PERSON\n",
      "699 97032 97035 CARDINAL\n",
      "2017 97466 97470 DATE\n",
      "Dallas 97520 97526 GPE\n",
      "Beka Chef 97749 97758 ORG\n",
      "Mauviel 97760 97767 PERSON\n",
      "Woll 97772 97776 ORG\n",
      "ü•òüç≤ # 97779 97783 PERSON\n",
      "SouthCarolina 98037 98050 ORG\n",
      "Georgia 98057 98064 GPE\n",
      "Nyc 98155 98158 GPE\n",
      "Miami 98160 98165 GPE\n",
      "Houston 98167 98174 GPE\n",
      "Chicago 98176 98183 GPE\n",
      "Nj 98185 98187 GPE\n",
      "# 98188 98189 DATE\n",
      "Hurricane Florence 98215 98233 EVENT\n",
      "# 98251 98252 CARDINAL\n",
      "MAGA 98252 98256 ORG\n",
      "the Blacksburg National Weather Service 98307 98346 ORG\n",
      "#labour # 98460 98469 MONEY\n",
      "# 98490 98491 DATE\n",
      "#snp 98496 98500 MONEY\n",
      "# 98506 98507 DATE\n",
      "#europe # 98527 98536 MONEY\n",
      "#ukpolitics 98545 98556 MONEY\n",
      "# 98590 98591 DATE\n",
      "#eupol 98604 98610 MONEY\n",
      "2 weeks 98621 98628 DATE\n",
      "üò™ 98700 98701 PERSON\n",
      "Bepannaah 98743 98752 PERSON\n",
      "Aditya 98786 98792 PERSON\n",
      "Zoya 98820 98824 PERSON\n",
      "EU 98918 98920 ORG\n",
      "Conservatives 98961 98974 NORP\n",
      "EU 99079 99081 ORG\n",
      "Brexit 99101 99107 PERSON\n",
      "Cardi 99207 99212 PERSON\n",
      "SerenaWilliams 99329 99343 ORG\n",
      "this 1st 99369 99377 DATE\n",
      "WalkAwayFromAllDemocrats 99454 99478 PERSON\n",
      "8 years 99499 99506 DATE\n",
      "MAGA 99603 99607 ORG\n",
      "AMERICANS 99663 99672 NORP\n",
      "#SupernaturalTNT #TheManWhoWouldBeKing 99677 99715 MONEY\n",
      "Crowley 99751 99758 PERSON\n",
      "Cas 99763 99766 PERSON\n",
      "Lisette 99886 99893 PERSON\n",
      "Conservatives 100129 100142 NORP\n",
      "#supremecourtjustice \n",
      " 100266 100288 MONEY\n",
      "Levi Strauss 100626 100638 PERSON\n",
      "#Entrepreneur | 100756 100771 MONEY\n",
      "Browning 101108 101116 PERSON\n",
      "FCS 101155 101158 ORG\n",
      "CERC 101188 101192 ORG\n",
      "Boyd 101200 101204 PERSON\n",
      "1 101208 101209 MONEY\n",
      "üíü 101366 101367 PERSON\n",
      "The Democratic Socialists of America 101399 101435 ORG\n",
      "America 101538 101545 GPE\n",
      "MAGA 101555 101559 ORG\n",
      "DeepState 101563 101572 EVENT\n",
      "GTFO 101759 101763 ORG\n",
      "2 MILLION 101781 101790 CARDINAL\n",
      "YouTube 101800 101807 ORG\n",
      "1 101826 101827 MONEY\n",
      "US 101835 101837 GPE\n",
      "Music Video Charts 101845 101863 ORG\n",
      "Greece 101902 101908 GPE\n",
      "Fiona 102024 102029 PERSON\n",
      "about 10 weeks old 102038 102056 DATE\n",
      "Chicago 102247 102254 GPE\n",
      "Democrat 102256 102264 NORP\n",
      "Amazon 102364 102370 ORG\n",
      "60% 102996 102999 PERCENT\n",
      "2030 103050 103054 DATE\n",
      "#ClimateAction # 103189 103205 MONEY\n",
      "#Communist # 103313 103325 MONEY\n",
      "the DC Chapter 103382 103396 LAW\n",
      "Democrat Socialists of America 103400 103430 ORG\n",
      "MAGA 103496 103500 ORG\n",
      "#ukbloggers # 103673 103686 MONEY\n",
      "#bbloggers # 103696 103708 MONEY\n",
      "William Murphy 103754 103768 PERSON\n",
      "SerenaWilliams 103816 103830 ORG\n",
      "first 103842 103847 ORDINAL\n",
      "Kim Clijsters 103941 103954 PERSON\n",
      "a year 103971 103977 DATE\n",
      "HALLOWEEN 104035 104044 DATE\n",
      "Trump 104162 104167 ORG\n",
      "India 104322 104327 GPE\n",
      "MAGA 104492 104496 ORG\n",
      "26 104545 104547 CARDINAL\n",
      "MSM 104625 104628 ORG\n",
      "#political # 104663 104675 MONEY\n",
      "#Emoluments # 104707 104720 MONEY\n",
      "#Law # 104752 104758 MONEY\n",
      "#MediaWatch # 104765 104778 MONEY\n",
      "MediaBias 104803 104812 PRODUCT\n",
      "first 104962 104967 ORDINAL\n",
      "#dontgiveyourmagictomuggles \n",
      "#Beeto 105198 105233 MONEY\n",
      "Texas 105280 105285 GPE\n",
      "MAGA 105289 105293 ORG\n",
      "#DVC \n",
      " 105312 105318 MONEY\n",
      "IOU 105319 105322 ORG\n",
      "ICO 105384 105387 ORG\n",
      "#Crypto # 105479 105488 MONEY\n",
      "Antifa 105740 105746 NORP\n",
      "Jeff 105764 105768 PERSON\n",
      "ACLU 105818 105822 ORG\n",
      "Antifa 105824 105830 NORP\n",
      "CNBC 105855 105859 ORG\n",
      "SugarDaddy Retweet 105864 105882 PERSON\n",
      "Budd 106093 106097 PERSON\n",
      "today 106169 106174 DATE\n",
      "6 # 106504 106507 DATE\n",
      "FBI 106507 106510 ORG\n",
      "JudgeKavanaugh 106535 106549 PRODUCT\n",
      "Fascists 106583 106591 NORP\n",
      "Anti Fascists 106629 106642 NORP\n",
      "Tennessee 106646 106655 GPE\n",
      "Sugardaddy Retweet 106781 106799 PERSON\n",
      "daily 106838 106843 DATE\n",
      "USA 106844 106847 GPE\n",
      "Canada 106848 106854 GPE\n",
      "UK 106859 106861 GPE\n",
      "only #sugarbabyneeded # 106862 106885 MONEY\n",
      "#findom # 106901 106910 MONEY\n",
      "Canada 106921 106927 GPE\n",
      "ANTIFA 106932 106938 ORG\n",
      "ü§î \n",
      " 107092 107095 PERSON\n",
      "#forcedintox 107202 107214 MONEY\n",
      "#coercedintox 107215 107228 MONEY\n",
      "#findom \n",
      " 107237 107246 MONEY\n",
      "DisarmThem 107247 107257 ORG\n",
      "Antifa 107289 107295 NORP\n",
      "#2A \n",
      "#StayOnYourPath 107519 107539 MONEY\n",
      "Colin 107897 107902 PERSON\n",
      "Antifa 108068 108074 NORP\n",
      "45 108162 108164 CARDINAL\n",
      "WNs 108294 108297 ORG\n",
      "NAB)JESUS 108422 108431 PERSON\n",
      "Antifa 108541 108547 NORP\n",
      "The Ontario Court of Appeal 108646 108673 ORG\n",
      "Toronto 108735 108742 GPE\n",
      "47 to 25 108765 108773 CARDINAL\n",
      "Gun Control 109032 109043 ORG\n",
      "Chicago 109062 109069 GPE\n",
      "#LiberalsHateAmerica \n",
      "@user 109074 109101 MONEY\n",
      "Naomi Osaka 109227 109238 PERSON\n",
      "üéæ 109242 109243 PERSON\n",
      "94-year-old 109557 109568 DATE\n",
      "Bletchley Park 109569 109583 FAC\n",
      "Nazis 109613 109618 NORP\n",
      "1941 109622 109626 DATE\n",
      "Supreme Court 109784 109797 ORG\n",
      "20+ years ago 109860 109873 DATE\n",
      "Anita Hill 109882 109892 PERSON\n",
      "Neanderthal 109923 109934 NORP\n",
      "WCE 109938 109941 ORG\n",
      "Trump 110375 110380 ORG\n",
      "# 110404 110405 DATE\n",
      "# 110412 110413 DATE\n",
      "#CharlottesvilleKKK 110427 110446 PERSON\n",
      "#GOP \n",
      " 110456 110462 MONEY\n",
      "MAGA 110477 110481 ORG\n",
      "QAnon 110483 110488 ORG\n",
      "today 110684 110689 DATE\n",
      "Democrats 110889 110898 NORP\n",
      "#Armed # 110938 110946 MONEY\n",
      "# 111064 111065 CARDINAL\n",
      "#killSkills #RightFuckingNOW \n",
      "#BoycottLevis 111092 111135 MONEY\n",
      "the late 1990s 111142 111156 DATE\n",
      "Levi 111158 111162 ORG\n",
      "1999 111222 111226 DATE\n",
      "100,000 111246 111253 MONEY\n",
      "PAX 111275 111278 ORG\n",
      "250,000 111295 111302 MONEY\n",
      "2000 111315 111319 DATE\n",
      "another $100,000 111324 111340 MONEY\n",
      "2001 111344 111348 DATE\n",
      "Social Justice 111461 111475 ORG\n",
      "NP 111571 111573 ORG\n",
      "#RadioENCHANTRESS \n",
      " 111630 111649 MONEY\n",
      "US 111686 111688 GPE\n",
      "3 days 111805 111811 DATE\n",
      "BBC Radio 2‚Äôs Festival 111819 111841 ORG\n",
      "a tonne 111966 111973 QUANTITY\n",
      "Conservatives 112224 112237 NORP\n",
      "Kennedy 112412 112419 PERSON\n",
      "EPA 112575 112578 ORG\n",
      "Puerto Rico 112592 112603 GPE\n",
      "more than 3,000 112618 112633 CARDINAL\n",
      "11 months later 112689 112704 DATE\n",
      "Puerto Rico 112706 112717 GPE\n",
      "Kavanaugh 112831 112840 ORG\n",
      "MSNBC 112957 112962 ORG\n",
      "Hardball 112964 112972 WORK_OF_ART\n",
      "Obama 113038 113043 PERSON\n",
      "Obama 113082 113087 PERSON\n",
      "White House 113110 113121 ORG\n",
      "Obama \n",
      " 113220 113227 PERSON\n",
      "XboxShare 113250 113259 ORG\n",
      "the People's Party of Canada 113487 113515 ORG\n",
      "Sonia Iqbal 113678 113689 PERSON\n",
      "Levi Strauss 113870 113882 PERSON\n",
      "America 114016 114023 GPE\n",
      "California 114356 114366 GPE\n",
      "Kavanaugh 114440 114449 PERSON\n",
      "#StopKavanaugh 114668 114682 MONEY\n",
      "Sam 114873 114876 PERSON\n",
      "Mike McCoy 115320 115330 PERSON\n",
      "Wilks 115332 115337 ORG\n",
      "GOP 115474 115477 ORG\n",
      "DiFi 115591 115595 PERSON\n",
      "the 13th hour 115617 115630 TIME\n",
      "Ford 115660 115664 ORG\n",
      "Dems 115714 115718 NORP\n",
      "Chicago 115764 115771 GPE\n",
      "1 115859 115860 MONEY\n",
      "RealSmart 115870 115879 ORG\n",
      "UBC 115925 115928 ORG\n",
      "the Real Estate Council of BC 115942 115971 ORG\n",
      "Florence 116057 116065 GPE\n",
      "one 116188 116191 CARDINAL\n",
      "India 116241 116246 GPE\n",
      "Scott Gottlieb 116448 116462 PERSON\n",
      "MassShhoting 116683 116695 ORG\n",
      "# 116730 116731 CARDINAL\n",
      "Barbara Boxer 116781 116794 PERSON\n",
      "35 Years Ago 117030 117042 DATE\n",
      "first 117112 117117 ORDINAL\n",
      "Matt Hardy 117321 117331 PERSON\n",
      "Randy Orton 117336 117347 PERSON\n",
      "one 117355 117358 CARDINAL\n",
      "NSWpol 117609 117615 ORG\n",
      "#Springst 117616 117625 PERSON\n",
      "Qldpol 117627 117633 ORG\n",
      "WApol 117635 117640 ORG\n",
      "one 117678 117681 CARDINAL\n",
      "DOJ 117751 117754 ORG\n",
      "LexisNexis 117853 117863 ORG\n",
      "Dem Socialists 117873 117887 ORG\n",
      "Antifa 117888 117894 NORP\n",
      "Veritas 117903 117910 ORG\n",
      "week of September 17th 118018 118040 DATE\n",
      "Sami 118084 118088 PERSON\n",
      "Hattie 118114 118120 PERSON\n",
      "Kayla 118235 118240 PERSON\n",
      "joe Biden‚Äôs 118298 118309 PERSON\n",
      "#maga 118327 118332 MONEY\n",
      "Chicago 118413 118420 GPE\n",
      "Baltimore 118424 118433 GPE\n",
      "March 118507 118512 DATE\n",
      "Antifa 118520 118526 NORP\n",
      "BBCNews 118657 118664 ORG\n",
      "NHS 118709 118712 ORG\n",
      "James Cleverly 118713 118727 PERSON\n",
      "Conservatives 118732 118745 NORP\n",
      "Chicago 118945 118952 GPE\n",
      "ACLU 119008 119012 ORG\n",
      "Antifa 119015 119021 NORP\n",
      "Nickelodeon 119078 119089 ORG\n",
      "KallysMashup 119097 119109 ORG\n",
      "Heart, 119129 119135 PERSON\n",
      "Goddessüíù 119164 119172 PRODUCT\n",
      "Star. 119216 119221 GPE\n",
      "# 119238 119239 CARDINAL\n",
      "üòç 119406 119407 PERSON\n",
      "üëå 119547 119548 NORP\n",
      "MAGA 119575 119579 ORG\n",
      "KAG2018 119581 119588 GPE\n",
      "U.S. 119589 119593 GPE\n",
      "EU 119602 119604 ORG\n",
      "Trump 119657 119662 ORG\n",
      "Antifa 119761 119767 NORP\n",
      "#KillahP. # 119786 119797 MONEY\n",
      "Greece 119797 119803 GPE\n",
      "Levi Strauss 119909 119921 PERSON\n",
      "America 120055 120062 GPE\n",
      "Navratilova 120167 120178 PERSON\n",
      "Martina 120260 120267 PERSON\n",
      "info@USER 120459 120468 PERSON\n",
      "#maassen # 120687 120697 MONEY\n",
      "LiberalsAreDesperate 120724 120744 EVENT\n",
      "#Twitter 120870 120878 MONEY\n",
      "#BlackOps4 # 120919 120931 MONEY\n",
      "Twitter 120977 120984 WORK_OF_ART\n",
      "#SupportAllStreamers \n",
      "# 121033 121056 MONEY\n",
      "Deutschland 121078 121089 GPE\n",
      "#üá© 121091 121093 MONEY\n",
      "Scorpio 121222 121229 PRODUCT\n",
      "AafiaSiddiqui 121317 121330 ORG\n",
      "the Daughter of # 121385 121402 WORK_OF_ART\n",
      "Pakistan 121402 121410 GPE\n",
      "32 121475 121477 CARDINAL\n",
      "12 121484 121486 CARDINAL\n",
      "Scarlett Johansson 121488 121506 PERSON\n",
      "V.S   121509 121514 ORG\n",
      "Emma Stone 121514 121524 PERSON\n",
      "ICYMI 121671 121676 ORG\n",
      "Anna Grey Barbour 121686 121703 PERSON\n",
      "Women‚Äôs Volleyball Team 121711 121734 ORG\n",
      "Anna 121736 121740 PERSON\n",
      "2018 121746 121750 DATE\n",
      "Terry Sanford 121751 121764 PERSON\n",
      "Anna 121868 121872 PERSON\n",
      "#BlackNGold 121897 121908 MONEY\n",
      "üñ§üíõ 121919 121921 MONEY\n",
      "GodsPraiseroom 121924 121938 ORG\n",
      "$6 million 122107 122117 MONEY\n",
      "Chicago 122338 122345 GPE\n",
      "decades 122370 122377 DATE\n",
      "every week 122408 122418 DATE\n",
      "FordTheatre   122422 122435 PERSON\n",
      "GoFundMe 122451 122459 NORP\n",
      "Comey 122631 122636 ORG\n",
      "#Strzok \n",
      "@user 122647 122661 MONEY\n",
      "CarbonTax 122909 122918 ORG\n",
      "Canadian 122964 122972 NORP\n",
      "Trudeau 123026 123033 PERSON\n",
      "2015 123041 123045 DATE\n",
      "2019 123140 123144 DATE\n",
      "Friday 123201 123207 DATE\n",
      "this weekend 123264 123276 DATE\n",
      "September 123278 123287 DATE\n",
      "ReadANewBookMonth 123292 123309 ORG\n",
      "this month 123429 123439 DATE\n",
      "Antifa 123456 123462 NORP\n",
      "1500 123548 123552 MONEY\n",
      "Conservatives 123594 123607 NORP\n",
      "Canadians 123629 123638 NORP\n",
      "#onpoli 123682 123689 MONEY\n",
      "NightMayor 123692 123702 ORG\n",
      "DT 123706 123708 ORG\n",
      "Antifa 123737 123743 NORP\n",
      "Crescent Heights 123765 123781 GPE\n",
      "Nenshi 123809 123815 ORG\n",
      "communist 123818 123827 NORP\n",
      "Hillary 123978 123985 PERSON\n",
      "@ least 16 123990 124000 CARDINAL\n",
      "#WWG1WGA # 124034 124044 MONEY\n",
      "MAGA 124044 124048 ORG\n",
      "RedTsunami 124050 124060 ORG\n",
      "AmericaFirst 124062 124074 ORG\n",
      "WalkAway 124076 124084 ORG\n",
      "FBI 124188 124191 ORG\n",
      "Antifa 124193 124199 NORP\n",
      "Vegas 124201 124206 GPE\n",
      "Denver 124218 124224 GPE\n",
      "CNN 124403 124406 ORG\n",
      "4 124427 124428 CARDINAL\n",
      "# 124440 124441 CARDINAL\n",
      "#Ireland 124453 124461 PERCENT\n",
      "1984 124498 124502 DATE\n",
      "#Savethe8th 124526 124537 MONEY\n",
      "#8thAmendment 124538 124551 MONEY\n",
      "#BlueWave 124579 124588 MONEY\n",
      "LovethemBoth 124615 124627 ORG\n",
      "#Repealthe8th \n",
      "@user 124634 124654 MONEY\n",
      "Chicago 124741 124748 GPE\n",
      "Democrat 124750 124758 NORP\n",
      "2018 124871 124875 DATE\n",
      "REPUBLICANS 124879 124890 NORP\n",
      "Conservatives 124903 124916 NORP\n",
      "Trump 124986 124991 ORG\n",
      "democrats 125046 125055 NORP\n",
      "Maine 125110 125115 GPE\n",
      "Kavanaugh 125242 125251 PERSON\n",
      "Susan Collins 125319 125332 PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    df_1 = (token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "    token.shape_,token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "\n",
    "def split_in_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [str(sent).strip() for sent in doc.sents]\n",
    "\n",
    "sentence_txt = split_in_sentences(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics: Entity types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_file = pd.read_csv('entity_types_test_txt.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entity = entity_file['token_ent_type_'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_file.groupby('token_ent_type_').count().sort_values(by='token_ent_type_', ascending=False)\n",
    "\n",
    "entity_file['freq'] = entity_file.groupby('token_ent_type_')['token_ent_type_'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'entity_file' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ea0376454c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentity_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_file' is not defined"
     ]
    }
   ],
   "source": [
    "entity_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_labels_and_sentences(textfile, labelfile):\n",
    "\n",
    "    # read textfile and labelfile into two separate dataframes\n",
    "    df_text = pd.read_csv(textfile + '.txt', header=None, skiprows = 0,\n",
    "        names=['tweet'], sep='\\t', quoting=3)\n",
    "    df_labels = pd.read_csv(labelfile + '.txt',header=None, skiprows = 0, names=['label'],\n",
    "        sep='\\t', quoting=3)\n",
    "\n",
    "    index_text = [x for x in range(1, len(df_text.values)+1)]\n",
    "\n",
    "    df_labels.insert(loc=0, column='id', value =index_text)\n",
    "    df_text.insert(loc=0, column='id', value =index_text)\n",
    "    final_df = df_text.merge(df_labels, on='id', how='left')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def class_distribution(dataframe):\n",
    "    x = dataframe['label'].value_counts()\n",
    "    barplot = seaborn.barplot(x.index, x)\n",
    "    return barplot\n",
    "\n",
    "# WORD-COUNT\n",
    "def word_count(dataframe_col):\n",
    "    dataframe_col['word_count'] = dataframe_col['tweet'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    avg_off_tweets = round(dataframe_col[dataframe_col['label']==1]['word_count'].mean(),3)\n",
    "    avg_non_off_tweets = round(dataframe_col[dataframe_col['label']==0]['word_count'].mean(),3) \n",
    "\n",
    "    return dataframe_col, avg_off_tweets, avg_non_off_tweets\n",
    "\n",
    "# CHARACTER-COUNT\n",
    "def char_count(dataframe_col):\n",
    "    dataframe_col['char_count'] = dataframe_col['tweet'].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    #the average characters in offensive tweets\n",
    "    avg_char_off = round(dataframe_col[dataframe_col['label']==1]['char_count'].mean(),3) \n",
    "\n",
    "    #the average characters in non-offensive tweets\n",
    "    avg_char_non_off = round(dataframe_col[dataframe_col['label']==0]['char_count'].mean(),3)\n",
    "\n",
    "    return dataframe_col, avg_char_off, avg_char_non_off\n",
    "\n",
    "def plot_word_count(dataframe):\n",
    "    # PLOTTING WORD-COUNT\n",
    "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "    dataframe_words=dataframe[dataframe['label']==1]['word_count']\n",
    "    ax1.hist(dataframe_words,color='red')\n",
    "    ax1.set_title('offensive tweets')\n",
    "    dataframe_words=dataframe[dataframe['label']==0]['word_count']\n",
    "    ax2.hist(dataframe_words,color='green')\n",
    "    ax2.set_title('non-offensive tweets')\n",
    "    fig.suptitle('Words per tweet')\n",
    "    plt.show()\n",
    "\n",
    "def missing_values(dataframe):\n",
    "    res = dataframe.isna().sum()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-94bd7eeb51b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#LEMMATIZATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Initialize the lemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mwl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# This is a helper function to map NTLK position tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "    \n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE VALIDATION DATASET INTO TRAIN AND TEST\n",
    "def split_dataset(dataframe):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataframe[\"tweet\"],dataframe      \n",
    "    [\"label\"],test_size=0.2,shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "    \n",
    "def tokenize_train_test(X_train, X_test):\n",
    "    #Word2Vec\n",
    "    # Word2Vec runs on tokenized sentences\n",
    "    X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "    X_test_tok= [nltk.word_tokenize(i) for i in X_test]\n",
    "\n",
    "    return X_train_tok, X_test_tok\n",
    "\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] \n",
    "        for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) \n",
    "        for words in X])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = merging_labels_and_sentences('datasets/offensive/val_text', 'datasets/offensive/val_labels')\n",
    "\n",
    "missing_val = missing_values(val_df)\n",
    "avg_labels_word = word_count(val_df)\n",
    "avg_labels_char = char_count(val_df)\n",
    "\n",
    "print(' Average Number of Words - Offensive Tweets: ', avg_labels_word[1],'\\n','Average Number of Words - Non-offensive Tweets: ', avg_labels_word[2])\n",
    "print(' Average Characters in Offensive Tweets: ', avg_labels_char[1],'\\n','Average Characters in Non-offensive Tweets: ', avg_labels_char[2])\n",
    "\n",
    "print('\\nNumber of missing values for each column\\n',missing_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "barplot = class_distribution(val_df)\n",
    "\n",
    "plot_word_count = plot_word_count(val_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(val_df)\n",
    "print(\"Training set: %d samples\" % len(X_train))\n",
    "print(\"Test set: %d samples\" % len(X_test))\n",
    "\n",
    "X_train_tok, X_test_tok = tokenize_train_test(X_train, X_test)\n",
    "\n",
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#val_df['clean_text_tok']=[nltk.word_tokenize(i) for i in val_df['tweet']]\n",
    "model = gensim.models.Word2Vec(val_df['tweet'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv))      \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train) \n",
    "\n",
    " #Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Gaussian Naive Bayes(w2v)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_w2v = GaussianNB()\n",
    "nb_w2v.fit(X_train_vectors_w2v, y_train) \n",
    "\n",
    "\n",
    " #Predict y value for test dataset\n",
    "y_predict_nb_w2v = nb_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob_nb_w2v = nb_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict_nb_w2v))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict_nb_w2v))\n",
    " \n",
    "fpr_nb_w2v, tpr_nb_w2v, thresholds_nb_w2v = roc_curve(y_test, y_prob_nb_w2v)\n",
    "roc_auc_nb_w2v = auc(fpr_nb_w2v, tpr_nb_w2v)\n",
    "print('AUC:', roc_auc_nb_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using RandomForrest (w2v)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_w2v = RandomForestClassifier(n_estimators = 100)\n",
    "clf_w2v.fit(X_train_vectors_w2v, y_train) \n",
    "\n",
    "\n",
    " #Predict y value for test dataset\n",
    "y_predict_clf_w2v = clf_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob_clf_w2v = clf_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict_clf_w2v))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict_clf_w2v))\n",
    " \n",
    "fpr_clf_w2v, tpr_clf_w2v, thresholds_clf_w2v = roc_curve(y_test, y_prob_clf_w2v)\n",
    "roc_auc_clf_w2v = auc(fpr_clf_w2v, tpr_clf_w2v)\n",
    "print('AUC:', roc_auc_clf_w2v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using RandomForrest (tf-idf)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_tfidf = RandomForestClassifier(n_estimators = 100)\n",
    "clf_tfidf.fit(X_train_vectors_tfidf, y_train) \n",
    "\n",
    "\n",
    " #Predict y value for test dataset\n",
    "y_predict_clf_tfidf = clf_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob_clf_tfidf = clf_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict_clf_tfidf))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict_clf_tfidf))\n",
    " \n",
    "fpr_clf_tfidf, tpr_clf_tfidf, thresholds_clf_tfidf = roc_curve(y_test, y_prob_clf_tfidf)\n",
    "roc_auc_clf_tfidf = auc(fpr_clf_tfidf, tpr_clf_tfidf)\n",
    "print('AUC:', roc_auc_clf_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_preprocessed_data(dataframe):\n",
    "    #Pre-processing the new dataset\n",
    "    dataframe['clean_text'] = dataframe['tweet'].apply(lambda x: finalpreprocess(x)) \n",
    "    \n",
    "    #preprocess the data\n",
    "    X_test=dataframe['clean_text'] \n",
    "\n",
    "    #converting words to numerical data using tf-idf\n",
    "    X_vector=tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    #use the best model to predict 'target' value for the new dataset \n",
    "    y_predict = lr_tfidf.predict(X_vector)      \n",
    "    y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "    dataframe['predict_prob']= y_prob\n",
    "    dataframe['label']= y_predict\n",
    "    final=dataframe[['clean_text','label']].reset_index(drop=True)\n",
    "    return final \n",
    "\n",
    "#model_prediction_preprocessed_data(val_df).to_csv('model_predict_proc_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_nonprocessed_data(dataframe):\n",
    "    X_test=dataframe['tweet'] \n",
    "\n",
    "    #converting words to numerical data using tf-idf\n",
    "    X_vector=tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    #use the best model to predict 'target' value for the new dataset \n",
    "    y_predict = lr_tfidf.predict(X_vector)      \n",
    "    y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "    dataframe['predict_prob']= y_prob\n",
    "    dataframe['label']= y_predict\n",
    "    final=dataframe[['tweet','label']].reset_index(drop=True)\n",
    "    return final \n",
    "\n",
    "#model_prediction_nonprocessed_data(val_df).to_csv('model_predict_nonproc_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW WITH TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(X):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    print ('Creating bag of words...')\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.  \n",
    "    \n",
    "    # In this example features may be single words or two consecutive words\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 ngram_range = (1,2), \\\n",
    "                                 max_features = 10000) \n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of \n",
    "    # strings. The output is a sparse array\n",
    "    train_data_features = vectorizer.fit_transform(X)\n",
    "    \n",
    "    # Convert to a NumPy array for easy of handling\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    \n",
    "    # tfidf transform\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_features = tfidf.fit_transform(train_data_features).toarray()\n",
    "\n",
    "    # Take a look at the words in the vocabulary\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "   \n",
    "    return vectorizer, vocab, train_data_features, tfidf_features, tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer, vocab, train_data_features, tfidf_features, tfidf  = (\n",
    "        create_bag_of_words(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(features, label):\n",
    "    print (\"Training the logistic regression model...\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    ml_model = LogisticRegression(C = 100,random_state = 0)\n",
    "    ml_model.fit(features, label)\n",
    "    print ('Finished')\n",
    "    return ml_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = train_logistic_regression(tfidf_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = vectorizer.transform(X_test)\n",
    "# Convert to numpy array\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tfidf_features = tfidf.fit_transform(test_data_features)\n",
    "# Convert to numpy array\n",
    "test_data_tfidf_features = test_data_tfidf_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = ml_model.predict(test_data_tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_identified_y = predicted_y == y_test\n",
    "accuracy = np.mean(correctly_identified_y) * 100\n",
    "print ('Accuracy = %.0f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
    "\n",
    "bow = dict()\n",
    "bow[\"train\"] = (count_vectorizer.fit_transform(X_train), y_train)\n",
    "bow[\"test\"]  = (count_vectorizer.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow[\"train\"][0].shape)\n",
    "print(bow[\"test\"][0].shape)\n"
   ]
  },
  {
   "source": [
    "## BOW - Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embeddings(vectors, clean_questions_tokens, generate_missing=False):\n",
    "    embeddings = clean_questions_tokens.apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = lr_classifier     # lr_classifier | lsvm_classifier | nb_classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classifier.fit(*embedding[\"train\"])\n",
    "y_predict = classifier.predict(embedding[\"test\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(embedding[\"test\"][1], y_predict)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm = confusion_matrix(embedding[\"test\"][1], y_predict)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Non-Offensive','Offensive'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Multiclass classification\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "emoji_df = merging_labels_and_sentences('datasets/emoji/test_text', 'datasets/emoji/test_labels')\n",
    "\n",
    "X = emoji_df['tweet']\n",
    "y = emoji_df['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train_em, X_test_em, y_train_em, y_test_em = train_test_split(\n",
    "    X, y, test_size = 0.1, random_state = 13)\n",
    "\n",
    "X_train_tok_em= [nltk.word_tokenize(i) for i in X_train_em]  \n",
    "X_test_tok_em= [nltk.word_tokenize(i) for i in X_test_em]\n",
    "#clf = OneVsRestClassifier(SVC()).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emoji = gensim.models.Word2Vec(X,min_count=1)\n",
    "w2v_emoji = dict(zip(model_emoji.wv.index_to_key, model_emoji.wv))      \n",
    "model_emoji.w = MeanEmbeddingVectorizer(w2v_emoji)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v_em = model_emoji.w.transform(X_train_tok_em)\n",
    "X_test_vectors_w2v_em = model_emoji.w.transform(X_test_tok_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Creating the SVM model\n",
    "#model = OneVsRestClassifier(SVC())\n",
    "emoji_SVM = OneVsRestClassifier(GaussianNB())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fitting the model with training data\n",
    "emoji_SVM.fit(X_train_vectors_w2v_em, y_train_em)\n",
    "   \n",
    "# Making a prediction on the test set\n",
    "prediction_SVM = emoji_SVM.predict(X_test_vectors_w2v_em)\n",
    "   \n",
    "# Evaluating the model\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test_em, prediction_SVM) * 100} %\\n\\n\")\n",
    "print(f\"Classification Report : \\n\\n{classification_report(y_test_em, prediction_SVM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_SVC = OneVsOneClassifier(GaussianNB())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fitting the model with training data\n",
    "emoji_SVC.fit(X_train_vectors_w2v_em, y_train_em)\n",
    "   \n",
    "# Making a prediction on the test set\n",
    "prediction_SVC = emoji_SVC.predict(X_test_vectors_w2v_em)\n",
    "   \n",
    "# Evaluating the model\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test_em, prediction_SVC) * 100} %\\n\\n\")\n",
    "\n",
    "print(f\"Classification Report : \\n\\n{classification_report(y_test_em, prediction_SVC)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS DISTRIBUTION FOR TRAIN DATA\n",
    "\n",
    "seaborn.barplot(x.index, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD-COUNT\n",
    "train_df['word_count'] = train_df['tweet'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(train_df[train_df['label']==1]['word_count'].mean()) #average number of words in offensive tweets\n",
    "print(train_df[train_df['label']==0]['word_count'].mean()) #average number of words in non-offensive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLOTTING WORD-COUNT\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "train_words=train_df[train_df['label']==1]['word_count']\n",
    "ax1.hist(train_words,color='red')\n",
    "ax1.set_title('offensive tweets')\n",
    "train_words=train_df[train_df['label']==0]['word_count']\n",
    "ax2.hist(train_words,color='green')\n",
    "ax2.set_title('non-offensive tweets')\n",
    "fig.suptitle('Words per tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHARACTER-COUNT\n",
    "train_df['char_count'] = train_df['tweet'].apply(lambda x: len(str(x)))\n",
    "\n",
    "print(train_df[train_df['label']==1]['char_count'].mean()) \n",
    "#the average characters in offensive tweets\n",
    "print(train_df[train_df['label']==0]['char_count'].mean()) #the average characters in non-offensive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df[\"tweet\"],train_df[\"label\"],test_size=0.2,shuffle=True)\n",
    "#Word2Vec\n",
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] \n",
    "        for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) \n",
    "        for words in X])\n",
    "\n",
    "\n",
    "train_df['clean_text_tok']=[nltk.word_tokenize(i) for i in train_df['tweet']]\n",
    "model = gensim.models.Word2Vec(train_df['clean_text_tok'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv))      \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train) \n",
    "\n",
    " #Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_preprocessed_data(dataframe):\n",
    "    #Pre-processing the new dataset\n",
    "    dataframe['clean_text'] = dataframe['tweet'].apply(lambda x: finalpreprocess(x)) \n",
    "    \n",
    "    #preprocess the data\n",
    "    X_test=dataframe['clean_text'] \n",
    "\n",
    "    #converting words to numerical data using tf-idf\n",
    "    X_vector=tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    #use the best model to predict 'target' value for the new dataset \n",
    "    y_predict = lr_tfidf.predict(X_vector)      \n",
    "    y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "    dataframe['predict_prob']= y_prob\n",
    "    dataframe['label']= y_predict\n",
    "    final=dataframe[['clean_text','label']].reset_index(drop=True)\n",
    "    return final \n",
    "\n",
    "#model_prediction_preprocessed_data(val_df).to_csv('model_predict_proc_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_nonprocessed_data(dataframe):\n",
    "    X_test=dataframe['tweet'] \n",
    "\n",
    "    #converting words to numerical data using tf-idf\n",
    "    X_vector=tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    #use the best model to predict 'target' value for the new dataset \n",
    "    y_predict = lr_tfidf.predict(X_vector)      \n",
    "    y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "    dataframe['predict_prob']= y_prob\n",
    "    dataframe['label']= y_predict\n",
    "    final=dataframe[['clean_text','label']].reset_index(drop=True)\n",
    "    return final \n",
    "\n",
    "#model_prediction_nonprocessed_data(val_df).to_csv('model_predict_nonproc_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['clean_text'] = val_df['tweet'].apply(lambda x: finalpreprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "    \n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "\n",
    "train_df['clean_text'] = train_df['tweet'].apply(lambda x: finalpreprocess(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_preprocessed_data(dataframe):\n",
    "    #Pre-processing the new dataset\n",
    "    dataframe['clean_text'] = dataframe['tweet'].apply(lambda x: finalpreprocess(x)) \n",
    "    \n",
    "    #preprocess the data\n",
    "    X_test=dataframe['clean_text'] \n",
    "\n",
    "    #converting words to numerical data using tf-idf\n",
    "    X_vector=tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    #use the best model to predict 'target' value for the new dataset \n",
    "    y_predict = lr_tfidf.predict(X_vector)      \n",
    "    y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "    dataframe['predict_prob']= y_prob\n",
    "    dataframe['label']= y_predict\n",
    "    final=dataframe[['clean_text','label']].reset_index(drop=True)\n",
    "    return final \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_nonprocessed_data(dataframe):\n",
    "    X_test=dataframe['tweet'] \n",
    "\n",
    "    #converting words to numerical data using tf-idf\n",
    "    X_vector=tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    #use the best model to predict 'target' value for the new dataset \n",
    "    y_predict = lr_tfidf.predict(X_vector)      \n",
    "    y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "    dataframe['predict_prob']= y_prob\n",
    "    dataframe['label']= y_predict\n",
    "    final=dataframe[['clean_text','label']].reset_index(drop=True)\n",
    "    return final \n",
    "\n"
   ]
  },
  {
   "source": [
    "model_test_cleaned_data = model_prediction_preprocessed_data(test)\n",
    "model_test_noncleaned_data = model_prediction_nonprocessed_data(test)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_test_cleaned_data['label'].to_csv('model_prediction_cleaned_testfile.csv')\n",
    "\n",
    "#model_test_noncleaned_data['label'].to_csv('model_prediction_noncleaned_testfile.csv')\n",
    "\n"
   ]
  },
  {
   "source": [
    "train_df['clean_text'] = train_df['tweet'].apply(lambda x: finalpreprocess(x))\n",
    "train_df.head()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "file = pd.read_csv('annotation_df_test_text.csv',usecols=[\"token_ent_type_\"], quoting=3)\n",
    "\n",
    "\n",
    "file.head()\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "test_txt_entity_types = file.dropna()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#test_txt_entity_types.to_csv(\"entity_types_test_txt.csv\") "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "test_txt_entity_types['token_ent_type_'].value_counts()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('normalizer', StandardScaler()), #Step1 - normalize data\n",
    "    ('clf', LogisticRegression()) #step2 - classifier\n",
    "])\n",
    "pipeline.steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use classification algorithm (i.e. Stochastic Logistic Regression) on training set, then assess model performance on test set\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log', penalty='l1')\n",
    "lr.fit(X_train_vectors_w2v, y_train)\n",
    "\n",
    "print(lr.score(X_train_vectors_w2v, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_probas = lr.predict_proba(X_test_vectors_w2v)[:,1]\n",
    "fpr,tpr,_ = roc_curve(y_test, pred_probas)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use classification algorithm (i.e. Stochastic Logistic Regression) on training set, then assess model performance on test set\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log', penalty='l1')\n",
    "lr.fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    "print(lr.score(X_train_vectors_tfidf, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_probas = lr.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "fpr,tpr,_ = roc_curve(y_test, pred_probas)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "#download mnist data and split into train and test sets\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "li = []\n",
    "for elm in  X_train_vectors_w2v:\n",
    "    li.append(elm.reshape(10,10,1))\n",
    "X_train_vectors_w2v = np.array(li)\n",
    "something = abs(X_train_vectors_w2v)\n",
    "maximum = np.amax(something)\n",
    "X_train_vectors_w2v = np.divide(something,maximum)\n",
    "#Test\n",
    "li = []\n",
    "for elm in  X_test_vectors_w2v:\n",
    "    li.append(elm.reshape(10,10,1))\n",
    "X_test_vectors_w2v = np.array(li)\n",
    "something = abs(X_test_vectors_w2v)\n",
    "maximum = np.amax(something)\n",
    "X_test_vectors_w2v = np.divide(something,maximum)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create model\n",
    "cat = Sequential()\n",
    "#add model layers\n",
    "cat.add(Conv2D(10, kernel_size=3, activation=\"relu\", input_shape=(10,10,1)))\n",
    "cat.add(Conv2D(5, kernel_size=3, activation=\"relu\"))\n",
    "cat.add(Flatten())\n",
    "cat.add(Dense(2, activation=\"softmax\"))\n",
    "cat.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cat.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "cat.fit(X_train_vectors_w2v, y_train, validation_data=(X_test_vectors_w2v, y_test), epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "li_em = []\n",
    "for elm in  X_train_vectors_w2v_em:\n",
    "    li_em.append(elm.reshape(10,10,1))\n",
    "X_train_vectors_w2v = np.array(li_em)\n",
    "something_em = abs(X_train_vectors_w2v_em)\n",
    "maximum_em = np.amax(something_em)\n",
    "X_train_vectors_w2v_em = np.divide(something_em,maximum_em)\n",
    "#Test\n",
    "li_em_test = []\n",
    "for elm in  X_test_vectors_w2v_em:\n",
    "    li_em_test.append(elm.reshape(10,10,1))\n",
    "X_test_vectors_w2v_em = np.array(li_em_test)\n",
    "something_em2 = abs(X_test_vectors_w2v_em)\n",
    "maximum_em2 = np.amax(something_em2)\n",
    "X_test_vectors_w2v_em = np.divide(something_em2,maximum_em2)\n",
    "\n",
    "y_train_em = to_categorical(y_train_em)\n",
    "y_test_em = to_categorical(y_test_em)\n",
    "y_train_em[0].shape\n",
    "y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "emoji = Sequential()\n",
    "#add model layers\n",
    "emoji.add(Conv2D(100,  kernel_size=4, activation=\"relu\", input_shape=(10,10,1)))\n",
    "emoji.add(Conv2D(69, kernel_size=4, activation=\"relu\", return_sequences=True))\n",
    "emoji.add(Flatten())\n",
    "emoji.add(Dense(20, activation=\"sigmoid\"))\n",
    "emoji.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "emoji.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "emoji.fit(X_train_vectors_w2v_em, y_train_em, validation_data=(X_test_vectors_w2v_em, y_test_em), epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TweetEval Tutorial",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "name": "python376jvsc74a57bd07ca953050fbd6db0e15562356b1a786d9418e582a94adc67b248dc4dbecd989f",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "7ca953050fbd6db0e15562356b1a786d9418e582a94adc67b248dc4dbecd989f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02f91bc3545b48808e4812d13f480875": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25a58cab43ef453d8b9de797925f32fa",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_db546face47140a59dcfc21539492c51",
      "value": " 150/150 [00:00&lt;00:00, 233B/s]"
     }
    },
    "0a1beafebb954a498d7a1cccc7cee445": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e5e9065e1ed4adbb1b451672ca4e793": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2151898d3e1c47de9e681c8b7b8779e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_627335e621884fb6957b7b6731339148",
       "IPY_MODEL_02f91bc3545b48808e4812d13f480875"
      ],
      "layout": "IPY_MODEL_6e236c3f5e084a108811f1e6a32cb990"
     }
    },
    "25a58cab43ef453d8b9de797925f32fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2adcfba8e99b4ceeae8cea6c4060c58d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b6e1410b17543afb5dbf619666e83bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3b1773e755ca402f98b5033ed4af1b7a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41b12ad69a684cb597594e60355253ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "498473ac68aa48c1ba4d5578543a8e2b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "627335e621884fb6957b7b6731339148": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a1beafebb954a498d7a1cccc7cee445",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b6e1410b17543afb5dbf619666e83bd",
      "value": 150
     }
    },
    "683e01b6cd98402783fe062f75177b68": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d41343536e54dd3804ace50e15b7953": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e236c3f5e084a108811f1e6a32cb990": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7045c78b1b164511b62e20bec70f1639": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "75528dba634e465992e5b52107f19032": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b1773e755ca402f98b5033ed4af1b7a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_eab919bdf6cb499b90a14c297bbc94dc",
      "value": " 456k/456k [00:01&lt;00:00, 251kB/s]"
     }
    },
    "77dad5843dbc43eab577b4cef273228d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca9214bf42bf49d99bcec60cc60c6e9c",
      "max": 498682569,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7045c78b1b164511b62e20bec70f1639",
      "value": 498682569
     }
    },
    "7a80cfb9bfda407c9ee1b9bd8b11a2f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7de2e7e4e8f44dccabc908334f7d1e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7ede88ead0d449a18ca343d47b723e87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88a7b0479e8c40b6aea4084f8039d4f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8c7d3e2bd2714e929d6859e15e83003f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91abbefb5e734d47a01158aebf46d7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9655bb1c674342cfb3ec38378a376729": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41b12ad69a684cb597594e60355253ae",
      "max": 779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88a7b0479e8c40b6aea4084f8039d4f6",
      "value": 779
     }
    },
    "a5c3c70d67124cd09894d3b52dc33e16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_683e01b6cd98402783fe062f75177b68",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e7aee59723a5469a9691322d6e124226",
      "value": " 779/779 [00:00&lt;00:00, 974B/s]"
     }
    },
    "b2d0e13e096c49cd9e25bb8a526b1ef7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9024a2af0984f0a99d7542077d5d183": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77dad5843dbc43eab577b4cef273228d",
       "IPY_MODEL_d647beae5bff452ebc5e4dbc9974d63e"
      ],
      "layout": "IPY_MODEL_2adcfba8e99b4ceeae8cea6c4060c58d"
     }
    },
    "c2d4ce2a084b4f19bf7ab624c0a853ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd2b26c80b8e4807a595ce1ff9881623",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7de2e7e4e8f44dccabc908334f7d1e21",
      "value": 456318
     }
    },
    "ca9214bf42bf49d99bcec60cc60c6e9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d647beae5bff452ebc5e4dbc9974d63e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e5e9065e1ed4adbb1b451672ca4e793",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6d41343536e54dd3804ace50e15b7953",
      "value": " 499M/499M [00:08&lt;00:00, 61.0MB/s]"
     }
    },
    "db546face47140a59dcfc21539492c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df23412503a1413aadacd254b573c315": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0847fcebda84c4d8b00b27234d35bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9655bb1c674342cfb3ec38378a376729",
       "IPY_MODEL_a5c3c70d67124cd09894d3b52dc33e16"
      ],
      "layout": "IPY_MODEL_8c7d3e2bd2714e929d6859e15e83003f"
     }
    },
    "e2f226a69f704168bbbc6fca9df15e23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2d0e13e096c49cd9e25bb8a526b1ef7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7a80cfb9bfda407c9ee1b9bd8b11a2f5",
      "value": " 899k/899k [00:02&lt;00:00, 323kB/s]"
     }
    },
    "e7aee59723a5469a9691322d6e124226": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e958eee9b3db48abae24a1f8be1b5d91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c2d4ce2a084b4f19bf7ab624c0a853ea",
       "IPY_MODEL_75528dba634e465992e5b52107f19032"
      ],
      "layout": "IPY_MODEL_498473ac68aa48c1ba4d5578543a8e2b"
     }
    },
    "eab919bdf6cb499b90a14c297bbc94dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f55b841175a54f7ea34306caf6d775cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df23412503a1413aadacd254b573c315",
      "max": 898822,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91abbefb5e734d47a01158aebf46d7ce",
      "value": 898822
     }
    },
    "f9c4c6ab3ba449e199d7035bff07a5db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f55b841175a54f7ea34306caf6d775cd",
       "IPY_MODEL_e2f226a69f704168bbbc6fca9df15e23"
      ],
      "layout": "IPY_MODEL_7ede88ead0d449a18ca343d47b723e87"
     }
    },
    "fd2b26c80b8e4807a595ce1ff9881623": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}