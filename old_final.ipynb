{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import string\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a dataframe into a single list \n",
    "#text is split into words defined by their space inbetween\n",
    "#words are inserted into list \n",
    "\n",
    "\n",
    "\n",
    "def words_list(text):\n",
    "    #words are inserted into list \n",
    "    corpus=[]\n",
    "    for row in text:\n",
    "        tokens = row[0].split(\" \")\n",
    "        for token in tokens:\n",
    "            corpus.append(token)\n",
    "    \n",
    "    \n",
    "    def vocabulary_list(corpus):\n",
    "        #initlialize the vocabulary\n",
    "        vocab = list(set(\" \".join(corpus)))\n",
    "        vocab.remove(' ')\n",
    "        return vocab\n",
    "      \n",
    "    \n",
    "    def split_words_char(corpus):\n",
    "        #split the word into characters\n",
    "        corpus = [\" \".join(token) for token in corpus]\n",
    "        #appending </w>\n",
    "        corpus=[token+' </w>' for token in corpus]\n",
    "        return corpus\n",
    "        \n",
    "    x,y = split_words_char(corpus), vocabulary_list(corpus)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def del_punctuations(words):\n",
    "    punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(punctuation_table).lower() for word in words]\n",
    "    return words\n",
    "\n",
    "def token_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenization(words, text):\n",
    "    tokens = word_tokenize(text[1:])\n",
    "    words = [token.lower() for token in tokens if token.isalpha()]   \n",
    "    return words\n",
    "\n",
    "\n",
    "def stop_words(words):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "import re\n",
    "\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", elem))  \n",
    "    return df\n",
    "\n",
    "\n",
    "def word_frequency(words):\n",
    "    frequency_words = collections.Counter(words)\n",
    "    \n",
    "    #convert counter object to dictionary\n",
    "    frequency_words_dict = dict(frequency_words)\n",
    "    res = dict(sorted(frequency_words_dict.items(), key=lambda item: item[1]))\n",
    "    return res\n",
    "\n",
    "def top_20_most_common_words(freq_words):\n",
    "    res = dict(Counter(freq_words).most_common(20))\n",
    "    return res\n",
    "\n",
    "def most_common_words(dictionary):\n",
    "    # Output a dict of most common words\n",
    "    return dict(sorted(dictionary.items(),key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def least_common_words(dictionary):\n",
    "    return sorted(list(dictionary.items()),key=lambda x: x[1])\n",
    "\n",
    "# number of tokens, sentences, average tokens, total unique tokens, total number of tokens after cleaning\n",
    "\n",
    "def basic_statistics(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    tokens = word_tokenize(text[1:])\n",
    "    words = [token.lower() for token in tokens if token.isalpha()]\n",
    "    average_tokens = round(len(words)/len(sents))\n",
    "    unique_tokens = set(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    final_tokens = []\n",
    "    for each in words:\n",
    "        if each not in stop_words:\n",
    "            final_tokens.append(each) \n",
    "    \n",
    "    return len(tokens), len(sents), average_tokens, len(unique_tokens), len(final_tokens)\n",
    "\n",
    "# Function for loglog plots\n",
    "def llplot(list_var, labels, title):\n",
    "    \"\"\"Function that takes a list of datasets, list of labels and a title as string, and plots a loglogplot, example:\n",
    "    llplot([offensive_freq_words_val, offensive_freq_words_train, offensive_freq_words_test], [\"val\", \"train\", \"test\"], \"Offensive dataset, loglog plot\")\n",
    "    \"\"\"\n",
    "    # Size of the figure:\n",
    "    plt.figure(figsize = (7,6))\n",
    "    # Iterating through the datasets:\n",
    "    for idx, i in enumerate(list_var):\n",
    "        y = np.log(list(most_common_words(i).values()))\n",
    "        x = np.log([i for i in range(1,len(y)+1)])\n",
    "        plt.scatter(x, y, label = labels[idx])\n",
    "    # Labeling title and axis:\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Rank of word\")\n",
    "    plt.ylabel(\"Frequency of word\")\n",
    "    # Plotting:\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1.1: TOKENIZATION\n",
    "_ splitting text files into words _\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text file: Emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in txt files: offensive/emoji.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'throwback', 'with', 'my', 'favourite', 'person', '@', 'Water', 'Wall', 'glam', 'on', '@user', 'yesterday', 'for', '#kcon', 'makeup', 'using', '@user', 'in', '#featherette,‚Ä¶', 'Democracy', 'Plaza', 'in', 'the', 'wake', 'of', 'a', 'stunning', 'outcome', '#Decision2016', '@', 'NBC', 'News', 'Then', '&amp;', 'Now.', 'VILO', '@', 'Walt', 'Disney', 'Magic', 'Kingdom', 'Who', 'never...', '@', 'A', 'Galaxy', 'Far', 'Far', 'Away', 'Dinner', 'in', 'FLA', 'tonight', '//', 'Pan-seared', 'salmon', 'over', 'couscous', 'veggie', 'salad', '#yum', '#dinner', '#florida', '#salmon‚Ä¶', \"It's\", 'my', 'fav', 'seniors', 'last', 'game', 'congrats', 'on', 'beating', 'west', '@', 'West', 'Salem‚Ä¶', 'I', 'got', 'to', 'to', 'go', 'formal', 'with', 'my', 'best', 'friend', '@', 'Phi', 'Mu', 'at', 'JSU', \"'Cause\", 'I', 'Miss', 'My', 'Little', 'Homies', '.#Throwback']\n"
     ]
    }
   ],
   "source": [
    "file_path_val = pathlib.Path(r'datasets/emoji/val_text.txt')\n",
    "\n",
    "with open(file_path_val, 'r',encoding=\"utf8\") as f:\n",
    "    emoji_text_val = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "emoji_words_val_txt = emoji_text_val[1:].split()\n",
    "\n",
    "print(emoji_words_val_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unday', 'afternoon', 'walking', 'through', 'Venice', 'in', 'the', 'sun', 'with', '@user', 'Ô∏è', 'Ô∏è', 'Ô∏è', '@', 'Abbot', 'Kinney,', 'Venice', 'Time', 'for', 'some', 'BBQ', 'and', 'whiskey', 'libations.', 'Chomp,', 'belch,', 'chomp!', '(@', \"Lucille's\", 'Smokehouse', 'Bar-B-Que)', 'Love', 'love', 'love', 'all', 'these', 'people', 'Ô∏è', 'Ô∏è', 'Ô∏è', '#friends', '#bff', '#celebrate', '#blessed', '#sundayfunday', '@', 'San‚Ä¶', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'Ô∏è', '@', 'Toys\"R\"Us', 'Man', 'these', 'are', 'the', 'funniest', 'kids', 'ever!!', 'That', 'face!', '#HappyBirthdayBubb', '@', 'FLIPnOUT', 'Xtreme', '#sandiego', '@', 'San', 'Diego,', 'California', 'My', 'little', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'Ô∏è', '#ObsessedWithMyDog', '@', 'Cafe', 'Solstice', 'Capitol', 'Hill', 'More', '#tinyepic', 'things', '#tinyepicwestern,', 'this', 'one', 'is', 'crazy', '@user', 'I', 'may', 'be', 'one', 'of', 'your‚Ä¶', 'Last']\n"
     ]
    }
   ],
   "source": [
    "file_path_train = pathlib.Path(r'datasets/emoji/train_text.txt')\n",
    "\n",
    "\n",
    "with open(file_path_train, 'r',encoding=\"utf8\") as f:\n",
    "    emoji_text_train = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "emoji_words_train_txt = emoji_text_train[1:].split()\n",
    "print(emoji_words_train_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'Pelham', 'Parkway', 'The', 'calm', 'before......', '|', 'w/', 'sofarsounds', '@user', '|', ':', 'B.', 'Hall.......#sofarsounds‚Ä¶', 'Just', 'witnessed', 'the', 'great', 'solar', 'eclipse', '@', 'Tampa,', 'Florida', 'This', 'little', 'lady', 'is', '26', 'weeks', 'pregnant', 'today!', 'Excited', 'for', 'baby', 'Cam', 'to', 'come!', '@', 'Springfield,‚Ä¶', 'Great', 'road', 'trip', 'views!', '@', 'Shartlesville,', 'Pennsylvania', 'CHRISTMAS', 'DEALS', 'BUY', 'ANY', '3', 'SMALL', 'POMADES', '1.5', 'OR', '1.7', 'OZ', 'RECEIVE', 'THE', 'F&amp;S', 'COLLECTOR', 'TIN', '&amp;', 'COMB‚Ä¶', 'the', '#sisterstunt', 'was', 'mad', 'real', 'last', 'night', '#MiaStaxxx', '#AndreaStaxxx', '#denverqueen', '#staxxxlife‚Ä¶', \"I'm\", 'starting', 'to', 'love', 'shooting', 'in', 'the', 'dark', '#brandonwolfel', '@', 'New', 'York,', 'New', 'York', 'Let', 'the', 'sun', 'shine', 'through', 'Ô∏è', '5x5', 'Feet', '#oilpainting', '#oiloncanvas', '#acrylicpainting']\n"
     ]
    }
   ],
   "source": [
    "file_path_test = pathlib.Path(r'datasets/emoji/test_text.txt')\n",
    "\n",
    "\n",
    "with open(file_path_test, 'r',encoding=\"utf8\") as f:\n",
    "    emoji_text_test = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "emoji_words_test_txt = emoji_text_test[1:].split()\n",
    "print(emoji_words_test_txt[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations from txt files\n",
    "_ meaning signs, spacing and other disturbing features. Alle words are then turned into lower cases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_text.txt:\n",
      " ['little', 'throwback', 'with', 'my', 'favourite', 'person', '', 'water', 'wall', 'glam', 'on', 'user', 'yesterday', 'for', 'kcon', 'makeup', 'using', 'user', 'in', 'featherette‚Ä¶', 'democracy', 'plaza', 'in', 'the', 'wake', 'of', 'a', 'stunning', 'outcome', 'decision2016', '', 'nbc', 'news', 'then', 'amp', 'now', 'vilo', '', 'walt', 'disney', 'magic', 'kingdom', 'who', 'never', '', 'a', 'galaxy', 'far', 'far', 'away', 'dinner', 'in', 'fla', 'tonight', '', 'panseared', 'salmon', 'over', 'couscous', 'veggie', 'salad', 'yum', 'dinner', 'florida', 'salmon‚Ä¶', 'its', 'my', 'fav', 'seniors', 'last', 'game', 'congrats', 'on', 'beating', 'west', '', 'west', 'salem‚Ä¶', 'i', 'got', 'to', 'to', 'go', 'formal', 'with', 'my', 'best', 'friend', '', 'phi', 'mu', 'at', 'jsu', 'cause', 'i', 'miss', 'my', 'little', 'homies', 'throwback'] \n",
      "\n",
      "\n",
      "val_train.txt:\n",
      " ['unday', 'afternoon', 'walking', 'through', 'venice', 'in', 'the', 'sun', 'with', 'user', 'Ô∏è', 'Ô∏è', 'Ô∏è', '', 'abbot', 'kinney', 'venice', 'time', 'for', 'some', 'bbq', 'and', 'whiskey', 'libations', 'chomp', 'belch', 'chomp', '', 'lucilles', 'smokehouse', 'barbque', 'love', 'love', 'love', 'all', 'these', 'people', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'friends', 'bff', 'celebrate', 'blessed', 'sundayfunday', '', 'san‚Ä¶', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'Ô∏è', '', 'toysrus', 'man', 'these', 'are', 'the', 'funniest', 'kids', 'ever', 'that', 'face', 'happybirthdaybubb', '', 'flipnout', 'xtreme', 'sandiego', '', 'san', 'diego', 'california', 'my', 'little', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'obsessedwithmydog', '', 'cafe', 'solstice', 'capitol', 'hill', 'more', 'tinyepic', 'things', 'tinyepicwestern', 'this', 'one', 'is', 'crazy', 'user', 'i', 'may', 'be', 'one', 'of', 'your‚Ä¶', 'last'] \n",
      "\n",
      "\n",
      "val_test.txt:\n",
      " ['n', 'pelham', 'parkway', 'the', 'calm', 'before', '', 'w', 'sofarsounds', 'user', '', '', 'b', 'hallsofarsounds‚Ä¶', 'just', 'witnessed', 'the', 'great', 'solar', 'eclipse', '', 'tampa', 'florida', 'this', 'little', 'lady', 'is', '26', 'weeks', 'pregnant', 'today', 'excited', 'for', 'baby', 'cam', 'to', 'come', '', 'springfield‚Ä¶', 'great', 'road', 'trip', 'views', '', 'shartlesville', 'pennsylvania', 'christmas', 'deals', 'buy', 'any', '3', 'small', 'pomades', '15', 'or', '17', 'oz', 'receive', 'the', 'famps', 'collector', 'tin', 'amp', 'comb‚Ä¶', 'the', 'sisterstunt', 'was', 'mad', 'real', 'last', 'night', 'miastaxxx', 'andreastaxxx', 'denverqueen', 'staxxxlife‚Ä¶', 'im', 'starting', 'to', 'love', 'shooting', 'in', 'the', 'dark', 'brandonwolfel', '', 'new', 'york', 'new', 'york', 'let', 'the', 'sun', 'shine', 'through', 'Ô∏è', '5x5', 'feet', 'oilpainting', 'oiloncanvas', 'acrylicpainting'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "emoji_cleaned_val_words = del_punctuations(emoji_words_val_txt)\n",
    "emoji_cleaned_train_words = del_punctuations(emoji_words_train_txt)\n",
    "emoji_cleaned_test_words = del_punctuations(emoji_words_test_txt)\n",
    "\n",
    "print('val_text.txt:\\n',emoji_cleaned_val_words[:100],'\\n')\n",
    "\n",
    "print('\\nval_train.txt:\\n',emoji_cleaned_train_words[:100], '\\n')\n",
    "\n",
    "print('\\nval_test.txt:\\n',emoji_cleaned_test_words[:100],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A little throwback with my favourite person @ Water Wall\\nglam on @user yesterday for #kcon makeup using @user in #featherette,‚Ä¶\\nDemocracy Plaza in the wake of a stunning outcome #Decision2016 @ NBC News\\nThen &amp; Now.', \"VILO @ Walt Disney Magic Kingdom\\nWho never... @ A Galaxy Far Far Away\\nDinner in FLA tonight // Pan-seared salmon over couscous veggie salad #yum #dinner #florida #salmon‚Ä¶\\nIt's my fav seniors last game congrats on beating west @ West Salem‚Ä¶\\nI got to to go formal with my best friend @ Phi Mu at JSU\\n'Cause I Miss My Little Homies .#Throwback #CousinLove @ Indiana University\\nBirthday Kisses @ Madison, Wisconsin\\nGreat time in Tuscaloosa with my girl!\", '@ Bryant‚ÄìDenny Stadium\\nA seguir aprendiendo del mundo de las berries (@ John F. Kennedy International Airport (JFK) in Queens, NY, NY)\\n#livinginparadise @ Ramada Plaza Beach Resort\\n‚Ä¢‚Ä¢In order for your life to be \"GREAT\" you must first learn to \"APPRECIATE\" it... ‚Ä¢‚Ä¢ #Ny‚Ä¶\\nThese girls!', '#seniors2016 #memories #werarab @ The Loft\\nSully has his own beach style!', \"Yep cowboy boots on the beach:)‚Ä¶\\nWho cares that it's Libra season I'm still celebrating Ti #TiDay‚Ä¶\\nI love my job @ Decorah, Iowa\\nBecause 3 more days @ Echo Beach\\nWe didn't stand a chance at all.\"]\n",
      "['Sunday afternoon walking through Venice in the sun with @user Ô∏è Ô∏è Ô∏è @ Abbot Kinney, Venice \\nTime for some BBQ and whiskey libations.', 'Chomp, belch, chomp!', '(@ Lucille\\'s Smokehouse Bar-B-Que) \\nLove love love all these people Ô∏è Ô∏è Ô∏è #friends #bff #celebrate #blessed #sundayfunday @ San‚Ä¶ \\nÔ∏è Ô∏è Ô∏è Ô∏è @ Toys\"R\"Us \\nMan these are the funniest kids ever!!', 'That face!', '#HappyBirthdayBubb @ FLIPnOUT Xtreme \\n#sandiego @ San Diego, California \\nMy little Ô∏è Ô∏è Ô∏è Ô∏è Ô∏è #ObsessedWithMyDog @ Cafe Solstice Capitol Hill \\nMore #tinyepic things #tinyepicwestern, this one is crazy @user I may be one of your‚Ä¶ \\nLast night Ô∏è @ Omnia Night Club At Caesars Palace \\nfriendship at its finest.']\n",
      "['en Pelham Parkway\\nThe calm before...... | w/ sofarsounds @user | : B.', 'Hall.......#sofarsounds‚Ä¶\\nJust witnessed the great solar eclipse @ Tampa, Florida\\nThis little lady is 26 weeks pregnant today!', 'Excited for baby Cam to come!', '@ Springfield,‚Ä¶\\nGreat road trip views!', \"@ Shartlesville, Pennsylvania\\nCHRISTMAS DEALS BUY ANY 3 SMALL POMADES 1.5 OR 1.7 OZ RECEIVE THE F&amp;S COLLECTOR TIN &amp; COMB‚Ä¶\\nthe #sisterstunt was mad real last night #MiaStaxxx #AndreaStaxxx #denverqueen #staxxxlife‚Ä¶\\nI'm starting to love shooting in the dark #brandonwolfel @ New York, New York\\nLet the sun shine through Ô∏è 5x5 Feet #oilpainting #oiloncanvas #acrylicpainting #acryliconcanvas‚Ä¶\\nStill bitch im trill never been no fiend :@vibesbygallo #mustard @ Connecticut\\nLines for days Ô∏è Ô∏è Ô∏è #dance #dancephotography #dancephotographer #sandiegodance @ Scripps‚Ä¶\\nSunrise.\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "emoji_sentences_val_txt = token_sentences(emoji_text_val)   \n",
    "emoji_sentences_train_txt = token_sentences(emoji_text_train)\n",
    "emoji_sentences_test_txt = token_sentences(emoji_text_test)   \n",
    "\n",
    "print(emoji_sentences_val_txt[:5])\n",
    "print(emoji_sentences_train_txt[:5])\n",
    "print(emoji_sentences_test_txt[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words in each tokenization variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in tokenization for val_text: 49591\n",
      "Number of words in tokenization for val_train: 460902\n",
      "Number of words in tokenization for val_test: 517054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emoji_token_val = tokenization(emoji_cleaned_val_words, emoji_text_val)\n",
    "emoji_token_train = tokenization(emoji_cleaned_train_words, emoji_text_train)\n",
    "emoji_token_test = tokenization(emoji_cleaned_test_words, emoji_text_test)\n",
    "\n",
    "print(f'Number of words in tokenization for val_text: {len(emoji_token_val)}')\n",
    "print(f'Number of words in tokenization for val_train: {len(emoji_token_train)}')\n",
    "print(f'Number of words in tokenization for val_test: {len(emoji_token_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords like 'and, or, of, is, had.... etc' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after removing Stop Words: 33580\n",
      "Number of words after removing Stop Words: 314127\n",
      "Number of words after removing Stop Words: 345189\n"
     ]
    }
   ],
   "source": [
    "emoji_cleaned_val_words = stop_words(emoji_token_val)\n",
    "emoji_cleaned_train_words = stop_words(emoji_token_train)\n",
    "emoji_cleaned_test_words = stop_words(emoji_token_test)\n",
    "\n",
    "print(f'Number of words after removing Stop Words: {len(emoji_cleaned_val_words)}')\n",
    "print(f'Number of words after removing Stop Words: {len(emoji_cleaned_train_words)}')\n",
    "print(f'Number of words after removing Stop Words: {len(emoji_cleaned_test_words)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in val_text:\n",
      " [('friends', 76), ('high', 76), ('much', 78), ('fun', 78), ('first', 79), ('school', 81), ('life', 82), ('thank', 82), ('thanks', 86), ('center', 87), ('california', 87), ('see', 87), ('get', 89), ('city', 91), ('great', 96), ('back', 96), ('ca', 98), ('little', 99), ('university', 103), ('beautiful', 106), ('christmas', 116), ('like', 118), ('birthday', 119), ('last', 120), ('one', 125), ('today', 133), ('best', 136), ('got', 137), ('time', 137), ('good', 137), ('beach', 147), ('york', 148), ('night', 151), ('park', 152), ('day', 223), ('happy', 225), ('amp', 245), ('new', 292), ('love', 389)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in train_text:\n",
      " [('city', 736), ('francisco', 746), ('great', 751), ('fun', 756), ('little', 778), ('see', 783), ('disneyland', 798), ('back', 836), ('family', 859), ('birthday', 862), ('thank', 893), ('get', 895), ('got', 940), ('park', 964), ('best', 969), ('hollywood', 979), ('beautiful', 984), ('like', 1009), ('las', 1027), ('last', 1046), ('la', 1098), ('good', 1205), ('one', 1234), ('new', 1240), ('time', 1282), ('christmas', 1320), ('night', 1370), ('today', 1420), ('vegas', 1530), ('ca', 1591), ('beach', 1713), ('angeles', 1731), ('san', 1760), ('los', 1800), ('happy', 1901), ('day', 1902), ('amp', 2135), ('love', 3449), ('california', 4346)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in test_text:\n",
      " [('always', 737), ('first', 747), ('home', 753), ('year', 755), ('tonight', 769), ('morning', 778), ('much', 779), ('thanks', 784), ('fun', 790), ('life', 864), ('thank', 864), ('city', 895), ('see', 897), ('back', 905), ('little', 935), ('birthday', 959), ('great', 964), ('family', 991), ('beautiful', 1019), ('last', 1029), ('get', 1104), ('best', 1105), ('got', 1133), ('park', 1196), ('like', 1244), ('beach', 1252), ('good', 1317), ('california', 1363), ('night', 1406), ('christmas', 1408), ('york', 1458), ('time', 1486), ('one', 1488), ('today', 1504), ('day', 2180), ('happy', 2514), ('amp', 2560), ('new', 3563), ('love', 3919)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "#returns frequency of each word\n",
    "\n",
    "\n",
    "\n",
    "emoji_freq_words_val = word_frequency(emoji_cleaned_val_words)\n",
    "emoji_freq_words_train = word_frequency(emoji_cleaned_train_words)\n",
    "emoji_freq_words_test = word_frequency(emoji_cleaned_test_words)\n",
    "\n",
    "\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in val_text:\\n',list(emoji_freq_words_val.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in train_text:\\n',list(emoji_freq_words_train.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in test_text:\\n',list(emoji_freq_words_test.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary list for offensive text files\n",
    "_Looking through vocabulary lists can help you find problems\n",
    "(especially tokens that only occur once or twice)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary for text_val.txt:\n",
      " ['„Çπ', 'S', 'V', '|', '„É©', 'r', ':', 'Z', 'Êñ∞', 'a', '!', 'K', '„Éº', '√©', '„Éñ', 'Ô∏è', 'l', 'B', 'F', 'n'] \n",
      "\n",
      "Vocabulary for text_train.txt:\n",
      " ['Ïä¨', 'Ïùº', 'Œ†', '„Éê', '√ë', '‚ù•', '√¶', 'ÔΩå', '√¢', 'ÔºØ', '„Åö', '„Å™', 'Êñ∞', '·¥º', '!', '‚âã', 'ÌÜ†', '·¥≥', '„Éñ', 'Ô∏è'] \n",
      "\n",
      "Vocabulary for text_test.txt:\n",
      " ['Î∂Å', '„Éê', '‚ù•', '‡∏π', 'ÎÇ®', '√¶', 'ÊóÖ', 'ÔΩå', '„Å™', '·¥º', '!', '„Ç¶', 'ÌÜ†', '·¥≥', 'Ô∏è', ' Ä', 'ÔΩñ', 'Ëäã', 'üÑ≤', '‚ì£'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "emoji_corpus_val, emoji_vocab_val = words_list(emoji_text_val)\n",
    "emoji_corpus_train, emoji_vocab_train = words_list(emoji_text_train)\n",
    "emoji_corpus_test, emoji_vocab_test = words_list(emoji_text_test)\n",
    "\n",
    "print('Vocabulary for text_val.txt:\\n', emoji_vocab_val[:20],'\\n')\n",
    "print('Vocabulary for text_train.txt:\\n', emoji_vocab_train[:20],'\\n')\n",
    "print('Vocabulary for text_test.txt:\\n', emoji_vocab_test[:20],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in txt files: offensive/val_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#offensive_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', '@user', 'WiiU', 'is', 'not', 'even', 'a', 'real', 'console.', '@user', '@user', '@user', 'If', 'he', 'is', 'from', 'AZ', 'I', 'would', 'put', 'my', 'money', 'on', 'sex', 'with', 'underage', 'kids.', '@user', 'I', 'thought', 'Canada', 'had', 'strict', 'gun', 'control.', 'Help', 'me', 'understand', 'what', 'is', 'happening.', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', 'Following', 'all', '#Maga', 'patriots', 'please', 'follow', 'back', 'üëç', '#LionsDen', 'ü¶Å', '#MAGA2KAG', 'üá∫üá∏', '1', 'Minute', 'of', 'Truth:', 'Gun', 'Control', 'via', '@user', '@user', '@user', '@user', 'We', 'could', 'help', 'if', 'you', 'are', 'London', 'based', 'üòä', '@user', '@user', 'There', 'r', '65', 'million', 'that', 'can', 'sign', 'to', 'the']\n"
     ]
    }
   ],
   "source": [
    "file_path_val = pathlib.Path(r'datasets/offensive/val_text.txt')\n",
    "\n",
    "with open(file_path_val, 'r',encoding=\"utf8\") as f:\n",
    "    offensive_text_val = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "offensive_words_val_txt = offensive_text_val[1:].split()\n",
    "\n",
    "print(offensive_words_val_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'Bono...', 'who', 'cares.', 'Soon', 'people', 'will', 'understand', 'that', 'they', 'gain', 'nothing', 'from', 'following', 'a', 'phony', 'celebrity.', 'Become', 'a', 'Leader', 'of', 'your', 'people', 'instead', 'or', 'help', 'and', 'support', 'your', 'fellow', 'countrymen.', '@user', 'Eight', 'years', 'the', 'republicans', 'denied', 'obama‚Äôs', 'picks.', 'Breitbarters', 'outrage', 'is', 'as', 'phony', 'as', 'their', 'fake', 'president.', '@user', 'Get', 'him', 'some', 'line', 'help.', 'He', 'is', 'gonna', 'be', 'just', 'fine.', 'As', 'the', 'game', 'went', 'on', 'you', 'could', 'see', 'him', 'progressing', 'more', 'with', 'his', 'reads.', 'He', 'brought', 'what', 'has', 'been', 'missing.', 'The', 'deep', 'ball', 'presence.', 'Now', 'he', 'just', 'needs', 'a', 'little', 'more', 'time', '@user', '@user', 'She', 'is', 'great.', 'Hi', 'Fiona!', '@user']\n"
     ]
    }
   ],
   "source": [
    "file_path_train = pathlib.Path(r'datasets/offensive/train_text.txt')\n",
    "\n",
    "\n",
    "with open(file_path_train, 'r',encoding=\"utf8\") as f:\n",
    "    offensive_text_train = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "offensive_words_train_txt = offensive_text_train[1:].split()\n",
    "print(offensive_words_train_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ibelieveblaseyford', 'is', 'liar', 'she', 'is', 'fat', 'ugly', 'libreal', '#snowflake', 'she', 'sold', 'her', 'herself', 'to', 'get', 'some', 'cash', '!!', 'From', 'dems', 'and', 'Iran', '!', 'Why', 'she', 'spoke', 'after', '#JohnKerryIranMeeting', '?', '@user', '@user', '@user', 'I', 'got', 'in', 'a', 'pretty', 'deep', 'debate', 'with', 'my', 'friend', 'and', 'she', 'told', 'me', 'that', 'latinos', 'for', 'Trump', 'and', 'blacks', 'for', 'Trump', 'were', 'paid', 'supporters', 'üòÇ', 'then', 'I', 'said', 'you', 'mean', 'antifa', 'are', 'paid', 'domestic', 'terrorist', 'and', 'she', 'said', 'No', 'they', 'are', 'anti-fascist', 'then', 'I', 'said', 'they', 'are', 'the', 'fascist', 'are', 'you', 'kidding', 'me?!', '...if', 'you', 'want', 'more', 'shootings', 'and', 'more', 'death,', 'then', 'listen', 'to', 'the', 'ACLU,', 'Black']\n"
     ]
    }
   ],
   "source": [
    "file_path_test = pathlib.Path(r'datasets/offensive/test_text.txt')\n",
    "\n",
    "\n",
    "with open(file_path_test, 'r',encoding=\"utf8\") as f:\n",
    "    offensive_text_test = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "offensive_words_test_txt = offensive_text_test[1:].split()\n",
    "print(offensive_words_test_txt[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations from txt files\n",
    "_ meaning signs, spacing and other disturbing features. Alle words are then turned into lower cases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_text.txt:\n",
      " ['user', 'user', 'wiiu', 'is', 'not', 'even', 'a', 'real', 'console', 'user', 'user', 'user', 'if', 'he', 'is', 'from', 'az', 'i', 'would', 'put', 'my', 'money', 'on', 'sex', 'with', 'underage', 'kids', 'user', 'i', 'thought', 'canada', 'had', 'strict', 'gun', 'control', 'help', 'me', 'understand', 'what', 'is', 'happening', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'user', 'following', 'all', 'maga', 'patriots', 'please', 'follow', 'back', 'üëç', 'lionsden', 'ü¶Å', 'maga2kag', 'üá∫üá∏', '1', 'minute', 'of', 'truth', 'gun', 'control', 'via', 'user', 'user', 'user', 'user', 'we', 'could', 'help', 'if', 'you', 'are', 'london', 'based', 'üòä', 'user', 'user', 'there', 'r', '65', 'million', 'that', 'can', 'sign', 'to', 'the'] \n",
      "\n",
      "\n",
      "val_train.txt:\n",
      " ['user', 'bono', 'who', 'cares', 'soon', 'people', 'will', 'understand', 'that', 'they', 'gain', 'nothing', 'from', 'following', 'a', 'phony', 'celebrity', 'become', 'a', 'leader', 'of', 'your', 'people', 'instead', 'or', 'help', 'and', 'support', 'your', 'fellow', 'countrymen', 'user', 'eight', 'years', 'the', 'republicans', 'denied', 'obama‚Äôs', 'picks', 'breitbarters', 'outrage', 'is', 'as', 'phony', 'as', 'their', 'fake', 'president', 'user', 'get', 'him', 'some', 'line', 'help', 'he', 'is', 'gonna', 'be', 'just', 'fine', 'as', 'the', 'game', 'went', 'on', 'you', 'could', 'see', 'him', 'progressing', 'more', 'with', 'his', 'reads', 'he', 'brought', 'what', 'has', 'been', 'missing', 'the', 'deep', 'ball', 'presence', 'now', 'he', 'just', 'needs', 'a', 'little', 'more', 'time', 'user', 'user', 'she', 'is', 'great', 'hi', 'fiona', 'user'] \n",
      "\n",
      "\n",
      "val_test.txt:\n",
      " ['ibelieveblaseyford', 'is', 'liar', 'she', 'is', 'fat', 'ugly', 'libreal', 'snowflake', 'she', 'sold', 'her', 'herself', 'to', 'get', 'some', 'cash', '', 'from', 'dems', 'and', 'iran', '', 'why', 'she', 'spoke', 'after', 'johnkerryiranmeeting', '', 'user', 'user', 'user', 'i', 'got', 'in', 'a', 'pretty', 'deep', 'debate', 'with', 'my', 'friend', 'and', 'she', 'told', 'me', 'that', 'latinos', 'for', 'trump', 'and', 'blacks', 'for', 'trump', 'were', 'paid', 'supporters', 'üòÇ', 'then', 'i', 'said', 'you', 'mean', 'antifa', 'are', 'paid', 'domestic', 'terrorist', 'and', 'she', 'said', 'no', 'they', 'are', 'antifascist', 'then', 'i', 'said', 'they', 'are', 'the', 'fascist', 'are', 'you', 'kidding', 'me', 'if', 'you', 'want', 'more', 'shootings', 'and', 'more', 'death', 'then', 'listen', 'to', 'the', 'aclu', 'black'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "offensive_cleaned_val_words = del_punctuations(offensive_words_val_txt)\n",
    "offensive_cleaned_train_words = del_punctuations(offensive_words_train_txt)\n",
    "offensive_cleaned_test_words = del_punctuations(offensive_words_test_txt)\n",
    "\n",
    "print('val_text.txt:\\n',offensive_cleaned_val_words[:100],'\\n')\n",
    "\n",
    "print('\\nval_train.txt:\\n',offensive_cleaned_train_words[:100], '\\n')\n",
    "\n",
    "print('\\nval_test.txt:\\n',offensive_cleaned_test_words[:100],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words in each tokenization variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in tokenization for val_text: 30416\n",
      "Number of words in tokenization for val_train: 258224\n",
      "Number of words in tokenization for val_test: 19619\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "offensive_token_val = tokenization(offensive_cleaned_val_words, offensive_text_val)\n",
    "offensive_token_train = tokenization(offensive_cleaned_train_words, offensive_text_train)\n",
    "offensive_token_test = tokenization(offensive_cleaned_test_words, offensive_text_test)\n",
    "\n",
    "print(f'Number of words in tokenization for val_text: {len(offensive_token_val)}')\n",
    "print(f'Number of words in tokenization for val_train: {len(offensive_token_train)}')\n",
    "print(f'Number of words in tokenization for val_test: {len(offensive_token_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords like 'and, or, of, is, had.... etc' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after removing Stop Words: 17155\n",
      "Number of words after removing Stop Words: 147302\n",
      "Number of words after removing Stop Words: 11080\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "offensive_cleaned_val_words = stop_words(offensive_token_val)\n",
    "offensive_cleaned_train_words = stop_words(offensive_token_train)\n",
    "offensive_cleaned_test_words = stop_words(offensive_token_test)\n",
    "\n",
    "print(f'Number of words after removing Stop Words: {len(offensive_cleaned_val_words)}')\n",
    "print(f'Number of words after removing Stop Words: {len(offensive_cleaned_train_words)}')\n",
    "print(f'Number of words after removing Stop Words: {len(offensive_cleaned_test_words)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in val_text:\n",
      " [('love', 31), ('great', 31), ('much', 32), ('democrats', 32), ('country', 32), ('could', 33), ('vote', 33), ('shit', 33), ('never', 35), ('believe', 35), ('way', 35), ('need', 36), ('say', 37), ('still', 38), ('time', 40), ('make', 40), ('go', 41), ('good', 42), ('even', 43), ('see', 44), ('right', 45), ('going', 47), ('us', 52), ('want', 55), ('would', 61), ('amp', 62), ('think', 68), ('trump', 69), ('one', 71), ('get', 73), ('know', 77), ('people', 89), ('maga', 98), ('conservatives', 107), ('like', 109), ('antifa', 118), ('control', 125), ('gun', 133), ('liberals', 137)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in train_text:\n",
      " [('better', 237), ('vote', 237), ('well', 240), ('much', 249), ('left', 252), ('still', 262), ('make', 276), ('way', 278), ('really', 284), ('love', 290), ('back', 290), ('say', 292), ('even', 294), ('going', 312), ('see', 318), ('shit', 319), ('never', 325), ('need', 326), ('want', 329), ('go', 340), ('us', 345), ('time', 349), ('right', 409), ('good', 416), ('think', 483), ('would', 507), ('know', 557), ('trump', 565), ('one', 568), ('get', 586), ('amp', 615), ('people', 830), ('conservatives', 839), ('maga', 907), ('like', 995), ('antifa', 1047), ('control', 1095), ('gun', 1230), ('liberals', 1260)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n",
      "Frequency of words in test_text:\n",
      " [('chicago', 18), ('keep', 19), ('even', 19), ('years', 21), ('go', 21), ('really', 21), ('think', 22), ('life', 22), ('still', 22), ('always', 23), ('support', 24), ('shit', 24), ('time', 25), ('please', 25), ('way', 26), ('democrats', 26), ('need', 27), ('kavanaugh', 28), ('never', 28), ('see', 29), ('going', 30), ('new', 30), ('know', 31), ('good', 31), ('via', 33), ('want', 37), ('love', 38), ('us', 42), ('trump', 44), ('people', 47), ('one', 48), ('get', 51), ('maga', 57), ('gun', 64), ('control', 64), ('like', 65), ('antifa', 74), ('conservatives', 80), ('liberals', 81)] \n",
      "\n",
      "________________________________________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "#returns frequency of each word\n",
    "offensive_freq_words_val = word_frequency(offensive_cleaned_val_words)\n",
    "offensive_freq_words_train = word_frequency(offensive_cleaned_train_words)\n",
    "offensive_freq_words_test = word_frequency(offensive_cleaned_test_words)\n",
    "\n",
    "\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in val_text:\\n',list(offensive_freq_words_val.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in train_text:\\n',list(offensive_freq_words_train.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in test_text:\\n',list(offensive_freq_words_test.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My approach to task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Heroin', 'is', 'my', 'passion', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the ideal tokenization from the library we were supposed to compare it with\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "sentence = 'Heroin is my passion.'\n",
    "\n",
    "tknzr.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\datasets\\\\offensive\\\\train_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ef5d9469208b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtoken_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\datasets\\\\offensive\\\\train_text.txt'"
     ]
    }
   ],
   "source": [
    "#Recreate\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "file_path_train = pathlib.Path(r'../datasets/offensive/train_text.txt')\n",
    "\n",
    "token_pattern = re.compile(r'\\w+')\n",
    "\n",
    "with open(file_path_train, 'r',encoding=\"utf8\") as f:\n",
    "    line = f.readline()\n",
    "    tokens = []\n",
    "    while line:\n",
    "        line = f.readline()\n",
    "        print(line)\n",
    "        print(\"OUR beta tokenizer\",re.findall(token_pattern,line))\n",
    "        print(\"DESIRED  tokenizer\",tknzr.tokenize(line))\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 02 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of our emoji vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of total words in the vocabulary \" + str(len(emoji_freq_words_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 most common tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emoji_top_20_val = top_20_most_common_words(emoji_freq_words_val)\n",
    "emoji_top_20_train = top_20_most_common_words(emoji_freq_words_train)\n",
    "emoji_top_20_test = top_20_most_common_words(emoji_freq_words_test)\n",
    "\n",
    "print('Top 20 in emoji_freq_words_val \\n',emoji_top_20_val, '\\n')\n",
    "print('Top 20 in emoji_freq_words_train \\n',emoji_top_20_train,'\\n')\n",
    "print('Top 20 in emoji_freq_words_test \\n',emoji_top_20_test,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 20 least common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emoji_least_words_val = least_common_words(emoji_freq_words_val)\n",
    "emoji_least_words_train = least_common_words(emoji_freq_words_train)\n",
    "emoji_least_words_test = least_common_words(emoji_freq_words_test)\n",
    "print('Top 20 least common words in emoji_freq_words_val \\n',emoji_least_words_val[0:20], '\\n')\n",
    "print('Top 20 least common words in emoji_freq_words_train \\n',emoji_least_words_train[0:20], '\\n')\n",
    "print('Top 20 least common words in emoji_freq_words_test \\n',emoji_least_words_test[0:20], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, Zipf's Law is a distribution of data, where the 2nd highest ranking has half the number of occurrences as the highest ranking, the 3rd having 1/3 number of occurrences and so on.\n",
    "Another way of writting Zipf's law is as following:\n",
    "rank x frequency $\\approx$ const\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way of determining weather something follows the law is to use the formula rank x frequency $\\approx$ const and plot the results as a histogram. If all the bars in the histogram has rougly the same height the given data follows Zipf's law\n",
    "Another way is to use a loglog plot. If the line follows a diagonal line, then there is evidence that the data follows Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Here we simply plot the distribution\n",
    "fig = plt.figure(figsize = (5,4))\n",
    "axes = fig.add_axes([0,0,1,1])\n",
    "axes.bar(emoji_top_20_val.keys(),emoji_top_20_val.values())\n",
    "fig.autofmt_xdate(rotation=45)\n",
    "axes.set_title('Emoji histogram of accidents based on words and occurrences')\n",
    "axes.set_ylabel('Count')\n",
    "axes.set_xlabel('Words');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we multiply the frequency with the rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li =  []\n",
    "counter = 1\n",
    "for elm in emoji_top_20_val.values():\n",
    "    li.append(elm * counter)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,4))\n",
    "axes = fig.add_axes([0,0,1,1])\n",
    "axes.bar(emoji_top_20_val.keys(),li)\n",
    "fig.autofmt_xdate(rotation=45)\n",
    "axes.set_title('Emoji histogram of accidents based on words and occurrences')\n",
    "axes.set_ylabel('Count')\n",
    "axes.set_xlabel('Words');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned dataset:\n",
    "llplot([emoji_freq_words_val, emoji_freq_words_train, emoji_freq_words_test], labels=[\"val\", \"train\", \"test\"], title=\"Cleaned Emoji dataset, loglog plot\")\n",
    "\n",
    "# Raw dataset\n",
    "emoji_raw_val = word_frequency(emoji_words_val_txt)\n",
    "emoji_raw_train = word_frequency(emoji_words_train_txt)\n",
    "emoji_raw_test = word_frequency(emoji_words_test_txt)\n",
    "\n",
    "llplot([emoji_raw_val, emoji_raw_train, emoji_raw_test], labels=[\"val\", \"train\", \"test\"], title=\"Raw Emoji dataset, loglog plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offensive dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of our offensive vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total words in the vocabulary \" + str(len(offensive_freq_words_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 most common tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offensive_top_20_val = top_20_most_common_words(offensive_freq_words_val)\n",
    "offensive_top_20_train = top_20_most_common_words(offensive_freq_words_train)\n",
    "offensive_top_20_test = top_20_most_common_words(offensive_freq_words_test)\n",
    "\n",
    "print('Top 20 in val_text.txt:\\n',offensive_top_20_val, '\\n')\n",
    "print('Top 20 in val_train.txt:\\n',offensive_top_20_train,'\\n')\n",
    "print('Top 20 in val_test.txt:\\n',offensive_top_20_test,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 20 least common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_least_words_val = least_common_words(offensive_freq_words_val)\n",
    "offensive_least_words_train = least_common_words(offensive_freq_words_train)\n",
    "offensive_least_words_test = least_common_words(offensive_freq_words_test)\n",
    "print('Top 20 least common words in offensive_freq_words_val \\n',offensive_least_words_val[0:20], '\\n')\n",
    "print('Top 20 least common words in offensive_freq_words_train \\n',offensive_least_words_train[0:20], '\\n')\n",
    "print('Top 20 least common words in offensive_freq_words_test \\n',offensive_least_words_test[0:20], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,4))\n",
    "axes = fig.add_axes([0,0,1,1])\n",
    "axes.bar(offensive_top_20_val.keys(),offensive_top_20_val.values())\n",
    "fig.autofmt_xdate(rotation=45)\n",
    "axes.set_title('Offensive histogram of accidents based on words and occurrences')\n",
    "axes.set_ylabel('Count')\n",
    "axes.set_xlabel('Words');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li =  []\n",
    "counter = 1\n",
    "for elm in offensive_top_20_val.values():\n",
    "    li.append(elm * counter)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,4))\n",
    "axes = fig.add_axes([0,0,1,1])\n",
    "axes.bar(offensive_top_20_val.keys(),li)\n",
    "fig.autofmt_xdate(rotation=45)\n",
    "axes.set_title('Emoji histogram of accidents based on words and occurrences')\n",
    "axes.set_ylabel('Count')\n",
    "axes.set_xlabel('Words');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned dataset:\n",
    "llplot([offensive_freq_words_val, offensive_freq_words_train, offensive_freq_words_test], labels=[\"val\", \"train\", \"test\"], title=\"Cleaned Offensive dataset, loglog plot\")\n",
    "\n",
    "# Raw dataset\n",
    "offensive_raw_val = word_frequency(offensive_words_val_txt)\n",
    "offensive_raw_train = word_frequency(offensive_words_train_txt)\n",
    "offensive_raw_test = word_frequency(offensive_words_test_txt)\n",
    "\n",
    "llplot([offensive_raw_val, offensive_raw_train, offensive_raw_test], labels=[\"val\", \"train\", \"test\"], title=\"Raw Offensive dataset, loglog plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an overall trend that the follows the law, however, this is has not been proven mathematicly, and our plots doesn't follow the excact distributions as described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data frame: Working with tweets by sentence in pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('datasets/offensive/train_text.txt', header=None, skiprows = 0, names=['tweet'], sep='\\t', quoting=3)\n",
    "print(\"Training Set:\"% train.columns, train.shape, len(train))\n",
    "\n",
    "\n",
    "test = pd.read_csv('datasets/offensive/test_text.txt',header=None, skiprows = 0, names=['tweet'], sep='\\t', quoting=3)\n",
    "print(\"Test Set:\"% test.columns, test.shape, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_train = [x for x in range(1, len(train.values)+1)]\n",
    "index_test = [x for x in range(1, len(test.values)+1)]\n",
    "\n",
    "train.insert(loc=0, column='id', value =index_train )\n",
    "test.insert(loc=0, column='id', value =index_test )\n",
    "\n",
    "train_labels = pd.read_csv('datasets/offensive/train_labels.txt',header=None, skiprows = 0, names=['label'], sep='\\t', quoting=3)\n",
    "train_labels.insert(loc=0, column='id', value=index_train)\n",
    "\n",
    "test_labels = pd.read_csv('datasets/offensive/test_labels.txt',header=None, skiprows = 0, names=['label'], sep='\\t', quoting=3)\n",
    "test_labels.insert(loc=0, column='id', value =index_test )\n",
    "\n",
    "\n",
    "test_df = test.merge(test_labels, on='id', how='left')\n",
    "train_df = train.merge(train_labels, on='id', how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = clean_text(test_df, 'tweet')\n",
    "train_clean = clean_text(train_df, 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "train_clean['tweet'] = train_clean['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "test_clean['tweet'] = test_clean['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean['tweet'] = test_clean['tweet'].apply(lambda x: word_tokenize(x))\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "test_clean_offensive = test_clean.loc[test_clean['label'] == 1]\n",
    "test_clean_offensive.head()\n",
    "test_clean_offensive['tweet'].to_csv('test_cleaned_off_lang_df.csv',  quoting=csv.QUOTE_NONE, escapechar=' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_nonoffensive = test_clean.loc[test_clean['label'] == 0]\n",
    "\n",
    "test_clean_nonoffensive['tweet'].to_csv('test_cleaned_non_off_lang_df.csv',  quoting=csv.QUOTE_NONE, escapechar=' ') \n",
    "\n",
    "test_clean_nonoffensive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_off_lists = test_clean_offensive['tweet'].values.tolist()\n",
    "test_off_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_non_off_lists = test_clean_nonoffensive['tweet'].values.tolist()\n",
    "test_non_off_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TweetEval Tutorial",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "7ca953050fbd6db0e15562356b1a786d9418e582a94adc67b248dc4dbecd989f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02f91bc3545b48808e4812d13f480875": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25a58cab43ef453d8b9de797925f32fa",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_db546face47140a59dcfc21539492c51",
      "value": " 150/150 [00:00&lt;00:00, 233B/s]"
     }
    },
    "0a1beafebb954a498d7a1cccc7cee445": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e5e9065e1ed4adbb1b451672ca4e793": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2151898d3e1c47de9e681c8b7b8779e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_627335e621884fb6957b7b6731339148",
       "IPY_MODEL_02f91bc3545b48808e4812d13f480875"
      ],
      "layout": "IPY_MODEL_6e236c3f5e084a108811f1e6a32cb990"
     }
    },
    "25a58cab43ef453d8b9de797925f32fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2adcfba8e99b4ceeae8cea6c4060c58d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b6e1410b17543afb5dbf619666e83bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3b1773e755ca402f98b5033ed4af1b7a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41b12ad69a684cb597594e60355253ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "498473ac68aa48c1ba4d5578543a8e2b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "627335e621884fb6957b7b6731339148": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a1beafebb954a498d7a1cccc7cee445",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b6e1410b17543afb5dbf619666e83bd",
      "value": 150
     }
    },
    "683e01b6cd98402783fe062f75177b68": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d41343536e54dd3804ace50e15b7953": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e236c3f5e084a108811f1e6a32cb990": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7045c78b1b164511b62e20bec70f1639": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "75528dba634e465992e5b52107f19032": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b1773e755ca402f98b5033ed4af1b7a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_eab919bdf6cb499b90a14c297bbc94dc",
      "value": " 456k/456k [00:01&lt;00:00, 251kB/s]"
     }
    },
    "77dad5843dbc43eab577b4cef273228d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca9214bf42bf49d99bcec60cc60c6e9c",
      "max": 498682569,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7045c78b1b164511b62e20bec70f1639",
      "value": 498682569
     }
    },
    "7a80cfb9bfda407c9ee1b9bd8b11a2f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7de2e7e4e8f44dccabc908334f7d1e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7ede88ead0d449a18ca343d47b723e87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88a7b0479e8c40b6aea4084f8039d4f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8c7d3e2bd2714e929d6859e15e83003f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91abbefb5e734d47a01158aebf46d7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9655bb1c674342cfb3ec38378a376729": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41b12ad69a684cb597594e60355253ae",
      "max": 779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88a7b0479e8c40b6aea4084f8039d4f6",
      "value": 779
     }
    },
    "a5c3c70d67124cd09894d3b52dc33e16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_683e01b6cd98402783fe062f75177b68",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e7aee59723a5469a9691322d6e124226",
      "value": " 779/779 [00:00&lt;00:00, 974B/s]"
     }
    },
    "b2d0e13e096c49cd9e25bb8a526b1ef7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9024a2af0984f0a99d7542077d5d183": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77dad5843dbc43eab577b4cef273228d",
       "IPY_MODEL_d647beae5bff452ebc5e4dbc9974d63e"
      ],
      "layout": "IPY_MODEL_2adcfba8e99b4ceeae8cea6c4060c58d"
     }
    },
    "c2d4ce2a084b4f19bf7ab624c0a853ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd2b26c80b8e4807a595ce1ff9881623",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7de2e7e4e8f44dccabc908334f7d1e21",
      "value": 456318
     }
    },
    "ca9214bf42bf49d99bcec60cc60c6e9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d647beae5bff452ebc5e4dbc9974d63e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e5e9065e1ed4adbb1b451672ca4e793",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6d41343536e54dd3804ace50e15b7953",
      "value": " 499M/499M [00:08&lt;00:00, 61.0MB/s]"
     }
    },
    "db546face47140a59dcfc21539492c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df23412503a1413aadacd254b573c315": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0847fcebda84c4d8b00b27234d35bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9655bb1c674342cfb3ec38378a376729",
       "IPY_MODEL_a5c3c70d67124cd09894d3b52dc33e16"
      ],
      "layout": "IPY_MODEL_8c7d3e2bd2714e929d6859e15e83003f"
     }
    },
    "e2f226a69f704168bbbc6fca9df15e23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2d0e13e096c49cd9e25bb8a526b1ef7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7a80cfb9bfda407c9ee1b9bd8b11a2f5",
      "value": " 899k/899k [00:02&lt;00:00, 323kB/s]"
     }
    },
    "e7aee59723a5469a9691322d6e124226": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e958eee9b3db48abae24a1f8be1b5d91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c2d4ce2a084b4f19bf7ab624c0a853ea",
       "IPY_MODEL_75528dba634e465992e5b52107f19032"
      ],
      "layout": "IPY_MODEL_498473ac68aa48c1ba4d5578543a8e2b"
     }
    },
    "eab919bdf6cb499b90a14c297bbc94dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f55b841175a54f7ea34306caf6d775cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df23412503a1413aadacd254b573c315",
      "max": 898822,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91abbefb5e734d47a01158aebf46d7ce",
      "value": 898822
     }
    },
    "f9c4c6ab3ba449e199d7035bff07a5db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f55b841175a54f7ea34306caf6d775cd",
       "IPY_MODEL_e2f226a69f704168bbbc6fca9df15e23"
      ],
      "layout": "IPY_MODEL_7ede88ead0d449a18ca343d47b723e87"
     }
    },
    "fd2b26c80b8e4807a595ce1ff9881623": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
