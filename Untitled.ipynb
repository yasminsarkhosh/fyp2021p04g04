{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "In C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Martin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer #for word embedding\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score # bag of words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import collections\n",
    "import csv\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import re, string\n",
    "import seaborn\n",
    "import string\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')#for model-building\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(dataframe):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataframe[\"tweet\"],dataframe      \n",
    "    [\"label\"],test_size=0.2,shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc_dist(x1, x2):\n",
    "    return np.sqrt(np.sum((x1-x2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_labels_and_sentences(textfile, labelfile):\n",
    "\n",
    "    # read textfile and labelfile into two separate dataframes\n",
    "    df_text = pd.read_csv(textfile + '.txt', header=None, skiprows = 0,\n",
    "        names=['tweet'], sep='\\t', quoting=3)\n",
    "    df_labels = pd.read_csv(labelfile + '.txt',header=None, skiprows = 0, names=['label'],\n",
    "        sep='\\t', quoting=3)\n",
    "\n",
    "    index_text = [x for x in range(1, len(df_text.values)+1)]\n",
    "\n",
    "    df_labels.insert(loc=0, column='id', value =index_text)\n",
    "    df_text.insert(loc=0, column='id', value =index_text)\n",
    "    final_df = df_text.merge(df_labels, on='id', how='left')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] \n",
    "        for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) \n",
    "        for words in X])\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] \n",
    "        for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) \n",
    "        for words in X])\n",
    "def tokenize_train_test(X_train, X_test):\n",
    "    #Word2Vec\n",
    "    # Word2Vec runs on tokenized sentences\n",
    "    X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "    X_test_tok= [nltk.word_tokenize(i) for i in X_test]\n",
    "\n",
    "    return X_train_tok, X_test_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = merging_labels_and_sentences('datasets/offensive/val_text', 'datasets/offensive/val_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rang = [i for i in range(1000)]\n",
    "val_df = val_df.drop(rang,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are splitting the data into training and test\n",
    "X_train, X_test, y_train, y_test = split_dataset(val_df)\n",
    "\n",
    "# Here we tokenize the data\n",
    "X_train_tok, X_test_tok = tokenize_train_test(X_train, X_test)\n",
    "\n",
    "# (tf-Idf)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "val_df['clean_text_tok']=[nltk.word_tokenize(i) for i in val_df['tweet']]\n",
    "model = gensim.models.Word2Vec(val_df['clean_text_tok'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv))      \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, K=3):\n",
    "        self.K = K\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.X_train = x_train\n",
    "        self.Y_train = y_train\n",
    "    def predict(self, X_test):\n",
    "        predictions = [] \n",
    "        for i in range(len(X_test)):\n",
    "            dist = np.array([euc_dist(X_test[i], x_t) for x_t in   \n",
    "            self.X_train])\n",
    "            dist_sorted = dist.argsort()[:self.K]\n",
    "            neigh_count = {}\n",
    "            for idx in dist_sorted:\n",
    "                if self.Y_train[idx] in neigh_count:\n",
    "                    neigh_count[self.Y_train[idx]] += 1\n",
    "                else:\n",
    "                    neigh_count[self.Y_train[idx]] = 1\n",
    "            sorted_neigh_count = sorted(neigh_count.items(),    \n",
    "            key=operator.itemgetter(1), reverse=True)\n",
    "            predictions.append(sorted_neigh_count[0][0]) \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07396527,  0.05435052, -0.02312874, ..., -0.07847989,\n",
       "         0.02893483, -0.02836217],\n",
       "       [-0.05662589,  0.03688555, -0.02470157, ..., -0.06327142,\n",
       "         0.02938491, -0.02798427],\n",
       "       [-0.05815583,  0.04000781, -0.02158228, ..., -0.06230677,\n",
       "         0.02461921, -0.02433636],\n",
       "       ...,\n",
       "       [-0.03886812,  0.03108108, -0.01321141, ..., -0.04781994,\n",
       "         0.01136416, -0.01751112],\n",
       "       [-0.02642599,  0.01954007,  0.00291754, ..., -0.03285357,\n",
       "        -0.00328902, -0.00503874],\n",
       "       [-0.04535031,  0.03086511, -0.00734934, ..., -0.0474283 ,\n",
       "         0.00972261, -0.01214844]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits()\n",
    "print(mnist.data.shape)\n",
    "X = mnist.data \n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_mnist, y_test_mnist = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "#print(X_train.shape, y_train.shape)\n",
    "#print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mnist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision \n",
      " [0.55263158 0.25925926]\n",
      "\n",
      "Recall \n",
      " [0.51219512 0.29166667]\n",
      "\n",
      "F-score \n",
      " [0.53164557 0.2745098 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = KNN(K = 3)\n",
    "model.fit(X_train_vectors_w2v, y_train)\n",
    "pred = model.predict(X_test_vectors_w2v)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, pred)\n",
    "print(\"Precision \\n\", precision)\n",
    "print(\"\\nRecall \\n\", recall)\n",
    "print(\"\\nF-score \\n\", fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_emoji = merging_labels_and_sentences('datasets/emoji/val_text', 'datasets/emoji/val_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rang = [i for i in range(2500)]\n",
    "val_df_emoji = val_df_emoji.drop(rang,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1001</td>\n",
       "      <td>Exchanging glances with bae like... #sloth @ T...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1002</td>\n",
       "      <td>Where dreams come true #Disney @ Magic Kingdom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1003</td>\n",
       "      <td>I'm lookin at that contour @user but y'all loo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1004</td>\n",
       "      <td>@ Boston, Massachusetts</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1005</td>\n",
       "      <td>The ones you don't remember taking, make the b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>don't know what I'd do without you @ Keene Sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>Senior night with my little Bailey !! So proud...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>Real friends or labeled as family! #BrotherMan...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>It makes me so happy meet people wearing hats ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>It's been 48 hours with her and she still hasn...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  label\n",
       "1000  1001  Exchanging glances with bae like... #sloth @ T...     14\n",
       "1001  1002     Where dreams come true #Disney @ Magic Kingdom      3\n",
       "1002  1003  I'm lookin at that contour @user but y'all loo...      1\n",
       "1003  1004                            @ Boston, Massachusetts      5\n",
       "1004  1005  The ones you don't remember taking, make the b...      2\n",
       "...    ...                                                ...    ...\n",
       "4995  4996  don't know what I'd do without you @ Keene Sta...      0\n",
       "4996  4997  Senior night with my little Bailey !! So proud...      3\n",
       "4997  4998  Real friends or labeled as family! #BrotherMan...      6\n",
       "4998  4999  It makes me so happy meet people wearing hats ...      3\n",
       "4999  5000  It's been 48 hours with her and she still hasn...      2\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are splitting the data into training and test\n",
    "X_train, X_test, y_train, y_test = split_dataset(val_df_emoji)\n",
    "\n",
    "# Here we tokenize the data\n",
    "X_train_tok, X_test_tok = tokenize_train_test(X_train, X_test)\n",
    "\n",
    "# (tf-Idf)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "val_df['clean_text_tok']=[nltk.word_tokenize(i) for i in val_df['tweet']]\n",
    "model = gensim.models.Word2Vec(val_df['clean_text_tok'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv))      \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = KNN(K = 10)\n",
    "model.fit(X_train_vectors_w2v, y_train)\n",
    "pred = model.predict(X_test_vectors_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.50      0.30       166\n",
      "           1       0.13      0.14      0.14        90\n",
      "           2       0.12      0.17      0.14        78\n",
      "           3       0.05      0.05      0.05        43\n",
      "           4       0.08      0.05      0.06        44\n",
      "           5       0.05      0.03      0.04        30\n",
      "           6       0.00      0.00      0.00        34\n",
      "           7       0.00      0.00      0.00        36\n",
      "           8       0.14      0.04      0.06        28\n",
      "           9       0.00      0.00      0.00        27\n",
      "          10       0.05      0.06      0.05        16\n",
      "          11       0.11      0.03      0.05        30\n",
      "          12       0.00      0.00      0.00        26\n",
      "          13       0.00      0.00      0.00        20\n",
      "          14       0.00      0.00      0.00        18\n",
      "          15       0.00      0.00      0.00        25\n",
      "          16       0.00      0.00      0.00        26\n",
      "          17       0.11      0.05      0.07        21\n",
      "          18       0.15      0.14      0.15        21\n",
      "          19       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.15       800\n",
      "   macro avg       0.06      0.06      0.06       800\n",
      "weighted avg       0.10      0.15      0.11       800\n",
      "\n",
      "Confusion Matrix: [[83 12 24  4  7  5  2  4  1  4  7  2  1  2  0  0  3  2  2  1]\n",
      " [32 13 16  7  2  3  4  0  1  2  4  0  0  2  0  1  0  1  2  0]\n",
      " [32 12 13  8  1  1  0  0  1  1  3  1  1  1  0  0  1  1  1  0]\n",
      " [22  3  7  2  0  1  0  0  0  1  1  2  0  1  0  0  0  1  0  2]\n",
      " [18  8  4  2  2  1  1  2  0  0  0  1  0  0  1  1  1  0  2  0]\n",
      " [12  5  6  2  2  1  0  0  1  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [22  5  1  1  0  0  0  0  1  1  0  0  0  1  0  0  0  1  1  0]\n",
      " [20  6  4  2  0  2  0  0  0  0  1  0  0  0  0  0  0  0  0  1]\n",
      " [16  4  3  2  0  0  0  0  1  1  0  0  0  0  0  0  0  1  0  0]\n",
      " [12  4  4  1  0  0  1  0  1  0  0  0  1  1  0  1  0  0  1  0]\n",
      " [ 4  6  0  0  2  0  1  0  0  0  1  0  1  0  0  0  0  0  1  0]\n",
      " [17  4  5  1  1  0  0  0  0  0  0  1  0  0  1  0  0  0  0  0]\n",
      " [ 8  3  4  2  0  3  1  0  0  0  1  1  0  0  0  0  1  1  1  0]\n",
      " [11  1  3  1  1  2  0  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 9  1  2  0  1  0  0  1  0  1  1  0  0  0  0  1  0  0  1  0]\n",
      " [15  2  2  0  1  1  0  0  0  0  0  1  0  0  1  0  0  0  2  0]\n",
      " [15  3  3  1  2  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [12  2  2  1  0  0  2  0  0  0  0  0  0  1  0  0  0  1  0  0]\n",
      " [11  2  3  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  3  0]\n",
      " [ 9  2  3  1  1  1  1  0  0  0  1  0  0  0  0  0  0  0  2  0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-0422d6c5bd1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Confusion Matrix:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AUC:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_prob' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,pred))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, pred))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fpr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1e24cfc9455a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m plt.plot(fpr, tpr, color='darkorange',\n\u001b[0m\u001b[0;32m      4\u001b[0m          lw=lw, label='ROC curve')\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'navy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fpr' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.datasets import mnist\n",
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create model\n",
    "knn = Sequential()\n",
    "#add model layers\n",
    "knn.add(Conv2D(10, kernel_size=3, activation=\"relu\", input_shape=(10,10,1)))\n",
    "knn.add(Conv2D(5, kernel_size=3, activation=\"relu\"))\n",
    "knn.add(Flatten())\n",
    "knn.add(Dense(2, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are splitting the data into training and test\n",
    "X_train, X_test, y_train, y_test = split_dataset(val_df)\n",
    "\n",
    "# Here we tokenize the data\n",
    "X_train_tok, X_test_tok = tokenize_train_test(X_train, X_test)\n",
    "\n",
    "# (tf-Idf)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "val_df['clean_text_tok']=[nltk.word_tokenize(i) for i in val_df['tweet']]\n",
    "model = gensim.models.Word2Vec(val_df['clean_text_tok'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv))      \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "li = []\n",
    "for elm in  X_train_vectors_w2v:\n",
    "    li.append(elm.reshape(10,10,1))\n",
    "X_train_vectors_w2v = np.array(li)\n",
    "something = abs(X_train_vectors_w2v)\n",
    "maximum = np.amax(something)\n",
    "X_train_vectors_w2v = np.divide(something,maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "li = []\n",
    "for elm in  X_test_vectors_w2v:\n",
    "    li.append(elm.reshape(10,10,1))\n",
    "X_test_vectors_w2v = np.array(li)\n",
    "something = abs(X_test_vectors_w2v)\n",
    "maximum = np.amax(something)\n",
    "X_test_vectors_w2v = np.divide(something,maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectors_w2v[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vectors_w2v[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(10, kernel_size=3, activation=\"relu\", input_shape=(10,10,1)))\n",
    "model.add(Conv2D(5, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "knn.fit(X_train_vectors_w2v, y_train, validation_data=(X_test_vectors_w2v, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
